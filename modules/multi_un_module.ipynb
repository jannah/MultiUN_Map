{
 "metadata": {
  "gist_id": "6726db02bf6bdfbc209a",
  "name": "",
  "signature": "sha256:36faf7f7c02eb828126e66fb15eceed1cd98f6a9e2ac6f1cd29741dfa0f20e5a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Multi United Nations Corpus Sever Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a list of all the service functions used to access and process the Multi UN corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "import os\n",
      "import chardet\n",
      "import string\n",
      "import inspect\n",
      "from unidecode import unidecode\n",
      "from progressbar import AnimatedMarker, Bar, BouncingBar,\\\n",
      "                            Counter, ETA, Percentage, ProgressBar, SimpleProgress, FileTransferSpeed\n",
      "from nltk.corpus import brown\n",
      "show_pbars = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Data Path"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FILENAME = inspect.getframeinfo(inspect.currentframe()).filename\n",
      "F_PATH = os.path.dirname(os.path.abspath(FILENAME))\n",
      "ROOT_CORPUS_DIR = ''\n",
      "RELATIVE_PATH_TO_XML = 'data/multiUN.en/un/xml/en'\n",
      "RELATIVE_PATH_TO_TXT = 'data/multiUN.en/un/txt/en'\n",
      "PATH_TO_FILES = os.path.abspath(os.path.join(F_PATH, '..', RELATIVE_PATH_TO_TXT))\n",
      "# PATH_TO_XML_FILES=os.path.join(\"..\",\"data\",\"multiUN.en\",\"un\",\"xml\",\"en\")\n",
      "PATH_TO_XML_FILES =  os.path.abspath(os.path.join(F_PATH, '..', RELATIVE_PATH_TO_XML))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fix Unicode and Incomplete Sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fix_unicode(s):\n",
      "    text = ''\n",
      "    try:\n",
      "        text = str(s)\n",
      "        return text\n",
      "    except:\n",
      "        try:\n",
      "#             print 'encoding:', chardet.detect(s)['encoding']\n",
      "            text = s.encode(chardet.detect(s)['encoding'])\n",
      "            return text\n",
      "        except Exception, e:\n",
      "#             print e\n",
      "            try:\n",
      "                text = unidecode(s)\n",
      "                return text\n",
      "#                 f.write(sent)\n",
      "            except Exception, e2:\n",
      "#                 print 'unidecode:', e2\n",
      "                print 'error: ', e2\n",
      "                print s\n",
      "                return text\n",
      "                pass\n",
      "INCOMPLETE_SUFFIXES = ['Mr.', 'Ms.', 'Ch.']\n",
      "def fix_incomplete_sentences(para):\n",
      "    \n",
      "    sents = []\n",
      "    for i in range(len(para)):\n",
      "        sent = para[i]\n",
      "        out_sent=para[i]\n",
      "        while any(sent.endswith(suffix) for suffix in INCOMPLETE_SUFFIXES) and i<(len(para)-1):\n",
      "            i+=1\n",
      "            out_sent+=' %s'%(para[i].strip())\n",
      "            sent=para[i]\n",
      "        sents.append(out_sent)\n",
      "    return sents\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_files(year = None, raw=True):\n",
      "    years = []\n",
      "    if year is None:\n",
      "        years = [ year for year in os.listdir(PATH_TO_FILES) if not '.txt' in year or '_OLD_' in year]\n",
      "    else:\n",
      "        years = [str(year)]\n",
      "    data = {}\n",
      "    pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(years)).start()\n",
      "    for i in range(len(years)):\n",
      "        y = years[i]\n",
      "        data.update(load_files_by_year(y, raw))\n",
      "        pbar.update(i+1)\n",
      "    pbar.finish()\n",
      "    return data\n",
      "\n",
      "\n",
      "def load_files_by_year(year, raw=True):\n",
      "    texts = []\n",
      "    file_type = 'raw' if raw else 'txt'\n",
      "    full_path = os.path.join(PATH_TO_FILES, year, file_type)        \n",
      "    files = os.listdir(full_path)\n",
      "    pbar2 = ProgressBar(widgets=[year, SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(files)).start()\n",
      "    for i in range(len(files)):\n",
      "        filename = os.path.join(full_path, files[i])\n",
      "        with open(filename, 'r') as f:\n",
      "            text = f.read()\n",
      "            text = text.decode('utf-8') #the regular text was throwing an exception complaining about ascii\n",
      "            texts.append(text) #Keeping each file in a seperate array element\n",
      "            pbar2.update(i+1)\n",
      "    pbar2.finish()\n",
      "    return texts    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Load XML Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree\n",
      "#data ={}\n",
      "def load_xml_files(year = None, path = PATH_TO_XML_FILES, show_pbar=show_pbars, content=True):\n",
      "    data = {}\n",
      "    years = []\n",
      "    if year is None:\n",
      "        years = [ year for year in os.listdir(path)]\n",
      "    else:\n",
      "        years = [str(year)]\n",
      "    \n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(years)).start()\n",
      "    for i in range(len(years)):\n",
      "        y = years[i]\n",
      "        data.update(load_xml_files_by_year(y, path, show_pbar, content))\n",
      "        if show_pbar:\n",
      "            pbar.update(i+1)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return data\n",
      "\n",
      "def load_xml_files_by_year(year, path = PATH_TO_XML_FILES, show_pbar = True, content=True):\n",
      "    documents = {}\n",
      "    full_path = os.path.join(path, year)    \n",
      "#     print full_path\n",
      "    files = [fname for fname in os.listdir(full_path) if fname.endswith('.xml')]\n",
      "    if show_pbar and len(files)>0:\n",
      "        pbar2 = ProgressBar(widgets=[year, SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(files)).start()\n",
      "    for i in range(len(files)):\n",
      "        f = files[i]\n",
      "        filename = os.path.join(full_path, f)\n",
      "        documents[f] = load_xml_file(filename=filename, content=conten)\n",
      "        if show_pbar:\n",
      "            pbar2.update(i+1)\n",
      "    if show_pbar and len(files)>0:\n",
      "        pbar2.finish()\n",
      "    return documents\n",
      "\n",
      "def get_year_from_filename(filename):\n",
      "    sep = '\\\\' if '\\\\' in filename else '/'\n",
      "    return str(filename.split(sep)[-2:-1][0])\n",
      "\n",
      "\n",
      "def load_xml_file(filename = None, content=True, year = None):\n",
      "    f = {}\n",
      "    tree = etree.parse(filename)\n",
      "    root = tree.getroot()\n",
      "    f=dict(root.attrib)\n",
      "    f['year'] = year if year else get_year_from_filename(filename)\n",
      "    if content:\n",
      "        xparas =  root.getchildren()[0].getchildren()[0].getchildren()\n",
      "        content = []\n",
      "        for xpara in xparas:\n",
      "          content.append([fix_unicode(sent.text.strip()) for sent in xpara if len(sent.text.strip())>0])\n",
      "        f['content'] =  content\n",
      "    return f\n",
      "\n",
      "# def flatten_document_structure_paragraphs(doc_dict):\n",
      "#     print 'flattening paragraphs'\n",
      "#     flat = [fix_incomplete_sentences(para) for doc in doc_dict for para in doc_dict[doc]['content']]\n",
      "#     return flat\n",
      "\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Load Document Map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json, zipfile\n",
      "RELATIVE_PATH_TO_MAP = 'util/MUN_MAP.zip'\n",
      "PATH_TO_MAP = os.path.abspath(os.path.join(F_PATH, '..', RELATIVE_PATH_TO_MAP))\n",
      "map_zip = zipfile.ZipFile(PATH_TO_MAP)\n",
      "MUN_MAP = None\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def load_doc_map():\n",
      "    global MUN_MAP\n",
      "    if MUN_MAP is None:\n",
      "        MUN_MAP = json.loads(map_zip.read('map.json','r'))\n",
      "    return MUN_MAP\n",
      "\n",
      "def validate_search_term(doc, term=None, doc_name=None, doc_id = None, doc_n=None, filename = None, title=None):\n",
      "    if doc==doc_name:\n",
      "        return True\n",
      "    if term is None and doc_name is not None:\n",
      "        term = doc_name\n",
      "    \n",
      "    return (term in MUN_MAP[doc]['attributes']['n'] or doc_n==MUN_MAP[doc]['attributes']['n']\n",
      "                 or term in MUN_MAP[doc]['attributes']['id'] or doc_id == MUN_MAP[doc]['attributes']['id']\n",
      "                 or doc.endswith(term)\n",
      "                 or \n",
      "                 ( 'scrape' in MUN_MAP[doc]  \n",
      "                 and (\n",
      "                        ( 'Title' in MUN_MAP[doc]['scrape'] and term.lower() in (MUN_MAP[doc]['scrape']['Title']).lower())\n",
      "                      or \n",
      "                        ( 'Subjects' in MUN_MAP[doc]['scrape'] and term.lower() in \" \".join(MUN_MAP[doc]['scrape']['Subjects']).lower())\n",
      "                      )\n",
      "                    )\n",
      "            )\n",
      "\n",
      "def load_contents(docs):\n",
      "    if docs is not None:\n",
      "        for doc in docs:\n",
      "            path = docs[doc]['attributes']['path']\n",
      "            doc_data = load_xml_file(filename = path, content=True)\n",
      "            if doc_data: \n",
      "                docs[doc]['content'] = doc_data\n",
      "    return docs\n",
      "\n",
      "\n",
      "def get_documents(term = None, doc_name = None, doc_id=None, doc_n=None, filename = None, title=None, limit = None, \n",
      "                  include_content = False):\n",
      "    load_doc_map()\n",
      "    if doc_name is not None and doc_name in MUN_MAP:\n",
      "            result =  {doc_name:MUN_MAP[doc_name]}\n",
      "    else:\n",
      "        term = term if term is not None else doc_name\n",
      "        result =  [(doc,MUN_MAP[doc]) for doc in MUN_MAP if validate_search_term(doc, term, doc_name, doc_id, doc_n, filename, title)]\n",
      "        if limit is not None:\n",
      "            result = result[:10]\n",
      "        result = dict(result)\n",
      "    if include_content:\n",
      "        result = load_contents(result)\n",
      "    return result\n",
      "\n",
      "def get_document(term = None, doc_name = None, doc_id=None, doc_n=None, filename = None, title=None, include_content = False):\n",
      "    load_doc_map()\n",
      "    if doc_name and doc_name in MUN_MAP:\n",
      "            result = {doc_name:MUN_MAP[doc_name]}\n",
      "    else:\n",
      "        term = term if term is not None else doc_name\n",
      "        result =  next({doc:MUN_MAP[doc]} for doc in MUN_MAP if validate_search_term(doc, term, doc_name, doc_id, doc_n, filename, title))\n",
      "\n",
      "    if include_content:\n",
      "        result = load_contents(result)\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_name = raw_input('Enter a document attribute:\\n')\n",
      "items = get_documents(doc_name=doc_name, limit = 10, include_content=True)\n",
      "print len(items), ' result%s'%('s' if len(items)>1 else '')\n",
      "items"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Enter a document attribute:\n",
        "multiUN.en\\un\\xml\\en\\2002\\A_56_1015-en.xml\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1  result\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "{'multiUN.en\\\\un\\\\xml\\\\en\\\\2002\\\\A_56_1015-en.xml': {u'attributes': {u'date': u'2002/07/31',\n",
        "   u'id': u'A/56/1015',\n",
        "   u'lang': u'English',\n",
        "   u'n': u'N0250355',\n",
        "   u'path': u'C:\\\\Users\\\\Hassan\\\\Documents\\\\iSchool\\\\NLP\\\\United Nations\\\\multiUN.en\\\\un\\\\xml\\\\en\\\\2002\\\\A_56_1015-en.xml'},\n",
        "  u'content': {'content': [['Fifty-sixth session'],\n",
        "    ['Agenda item 97'],\n",
        "    ['Sustainable development and international economic cooperation'],\n",
        "    ['Letter dated 26 July 2002 from the Permanent Representative of Fiji to the United Nations addressed to the Secretary-General'],\n",
        "    ['I have the honour to enclose herewith a copy of the Nadi Declaration: ACP Solidarity in a Globalised World, which was adopted at the third Summit of the African, Caribbean and Pacific Heads of State and Government that was held in Fiji on 18 and 19 July 2002 (see annex).'],\n",
        "    ['I have further the honour to request that the Nadi Declaration be disseminated to all Member States as a document of the fifty-sixth session of the General Assembly, under agenda item 97.'],\n",
        "    ['(Signed) A. Naidu Ambassador Permanent Representative'],\n",
        "    ['Annex to the letter dated 26 July 2002 from the Permanent Representative of Fiji to the United Nations addressed to the Secretary-General'],\n",
        "    ['Nadi Declaration ACP Solidarity in a Globalised World'],\n",
        "    ['Adopted at the third Summit of the African, Caribbean and Pacific Heads of State and Government'],\n",
        "    ['Denarau, Nadi, 18 and 19 July 2002'],\n",
        "    ['Balance prepared for offset.']],\n",
        "   'date': '2002/07/31',\n",
        "   'id': 'A/56/1015',\n",
        "   'lang': 'English',\n",
        "   'n': 'N0250355',\n",
        "   'year': '2002'},\n",
        "  u'links': [],\n",
        "  u'scrape': {u'Agenda Items': u'97',\n",
        "   u'Area': u'UNDOC',\n",
        "   u'Display PDF File': [{u'lang': u'English(874.5K)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/55/img/N0250355.pdf?OpenElement'},\n",
        "    {u'lang': u'French(147K)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/56/pdf/N0250356.pdf?OpenElement'},\n",
        "    {u'lang': u'Russian(250.3K)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/57/pdf/N0250357.pdf?OpenElement'},\n",
        "    {u'lang': u'Spanish(149K)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/58/pdf/N0250358.pdf?OpenElement'},\n",
        "    {u'lang': u'Chinese(368.4K)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/54/pdf/N0250354.pdf?OpenElement'}],\n",
        "   u'Distribution': u'GEN',\n",
        "   u'Download File': [{u'lang': u'English(39.5K)(WORD6)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/55/doc/N0250355.DOC?OpenElement'},\n",
        "    {u'lang': u'French(92K)(WORD6)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/56/doc/N0250356.DOC?OpenElement'},\n",
        "    {u'lang': u'Russian(128K)(WORD6)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/57/doc/N0250357.DOC?OpenElement'},\n",
        "    {u'lang': u'Spanish(107.5K)(WORD6)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/58/doc/N0250358.DOC?OpenElement'},\n",
        "    {u'lang': u'Chinese(70K)(WORD6)',\n",
        "     u'link': u'http://documents-dds-ny.un.org/doc/UNDOC/GEN/N02/503/54/doc/N0250354.DOC?OpenElement'}],\n",
        "   u'ODS': u'A/56/1015',\n",
        "   u'Publication date': u'31/7/2002',\n",
        "   u'Session/Year': u'56',\n",
        "   u'Subjects': [u'GLOBALIZATION',\n",
        "    u'POLITICAL VIOLENCE',\n",
        "    u'ECONOMIC DEVELOPMENT',\n",
        "    u'SUSTAINABLE DEVELOPMENT',\n",
        "    u'DECLARATIONS (TEXT)',\n",
        "    u'TRANSNATIONAL CRIME',\n",
        "    u'INTERNATIONAL TRADE',\n",
        "    u'TRADE LIBERALIZATION',\n",
        "    u'DEBT RELIEF',\n",
        "    u'INVESTMENTS',\n",
        "    u'ENVIRONMENTAL PROTECTION',\n",
        "    u'NEW TECHNOLOGIES',\n",
        "    u'TELECOMMUNICATIONS',\n",
        "    u'ECONOMIC, SOCIAL AND CULTURAL RIGHTS',\n",
        "    u'COOPERATION BETWEEN ORGANIZATIONS',\n",
        "    u'ECONOMIC COOPERATION',\n",
        "    u'DEVELOPMENT FINANCE'],\n",
        "   u'Symbol': u'A/56/1015',\n",
        "   u'Title': u'LETTER DATED 2002/07/26 FROM THE PERMANENT REPRESENTATIVE OF FIJI TO THE UNITED NATIONS ADDRESSED TO THE SECRETARY-GENERAL',\n",
        "   u'jobs': [{u'jobno': u'N0250355',\n",
        "     u'lang': u'English',\n",
        "     u'release_date': u'14/8/2002'},\n",
        "    {u'jobno': u'N0250356', u'lang': u'French', u'release_date': u'14/8/2002'},\n",
        "    {u'jobno': u'N0250357',\n",
        "     u'lang': u'Russian',\n",
        "     u'release_date': u'14/8/2002'},\n",
        "    {u'jobno': u'N0250358',\n",
        "     u'lang': u'Spanish',\n",
        "     u'release_date': u'14/8/2002'},\n",
        "    {u'jobno': u'N0250353', u'lang': u'Arabic', u'release_date': u'14/8/2002'},\n",
        "    {u'jobno': u'N0250354',\n",
        "     u'lang': u'Chinese',\n",
        "     u'release_date': u'14/8/2002'}]}}}"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Extract Paragraphs and Sentences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions to extract sentence or paragraph-sentence lists from document dictionary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_paragraphs(doc_dict, merge_paragraphs=False):\n",
      "    flat = [fix_incomplete_sentences(para) for doc in doc_dict for para in doc_dict[doc]['content']]\n",
      "    if merge_paragraphs:\n",
      "        flat = [\" \".join(para) for para in flat]\n",
      "    return flat\n",
      "\n",
      "def extract_sentences(doc_dict):\n",
      "    print 'flattening'\n",
      "    text = [fix_incomplete_sentences(para) for doc in doc_dict for para in doc_dict[doc]['content']]\n",
      "    text = [sent for para in text for sent in para]\n",
      "        \n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentence Tokenizers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def parse_sentences_from_text(text, use_nltk_tokenizer = False ):\n",
      "    sents = []\n",
      "    if use_nltk_tokenizer:\n",
      "\n",
      "        sents = sent_tokenizer.sentences_from_text(text)\n",
      "    else:\n",
      "        #sents = \"\\n\".join(texts) # join all the documents into one big string\n",
      "        #split the document by \\n and remove any empty line and extra whitespace\n",
      "        sents = [sent.strip() for sent in text.split('\\n') if len(sent.strip())>0] \n",
      "    return sents\n",
      "def parse_sentences_from_text2(text, use_nltk_tokenizer = False ):\n",
      "    sents = []\n",
      "    global sent_tokenizer\n",
      "    if use_nltk_tokenizer:\n",
      "        sents = sent_tokenizer.sentences_from_text(text)\n",
      "    else:\n",
      "        #sents = \"\\n\".join(texts) # join all the documents into one big string\n",
      "        #split the document by \\n and remove any empty line and extra whitespace\n",
      "        paragraphs = [sent.strip() for sent in text.split('\\n') if len(sent.strip())>0] \n",
      "        #some sentence boundaries are wrong. Some text is split on numbered lists and other abbreviations. hence the join\n",
      "        sents = [\" \".join(sent) for sent in [sent_tokenizer.sentences_from_text(para) for para in paragraphs]]\n",
      "    return sents\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sentence Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentence_count(sentences):\n",
      "    return len(sentences)\n",
      "\n",
      "def get_average_words_per_sentence(sentences):\n",
      "    \n",
      "    return sum([len(sent.split(' ')) for sent in sentences])/float(len(sentences))\n",
      "\n",
      "def get_average_characters_per_sentence(sentences):\n",
      "    return sum([len(sent) for sent in sentences])/float(len(sentences))\n",
      "\n",
      "def get_longest_sentence_by_words(sentences):\n",
      "    return max(sentences, key=lambda w:len(w)).split(\" \")\n",
      "\n",
      "def get_longest_sentence_by_characters(sentences):\n",
      "    return max(sentences, key=lambda w:len(w))\n",
      "\n",
      "def print_sentence_statistics(sentences):\n",
      "    print '%-32s' % 'Info type', '%-16s' % 'Value',  '\\n----------------------------------------'\n",
      "    print '%-32s' % 'number of sentences', '%-16d' % len(sentences)\n",
      "    print '%-32s' % 'average sentence length(words)', '%-16.2f' % get_average_words_per_sentence(sentences)\n",
      "    print '%-32s' % 'average sentence length(letters)', '%-16.2f' % get_average_characters_per_sentence(sentences)\n",
      "    print '%-32s' % 'longest sentence (words)', '%-16s' % len(get_longest_sentence_by_words(sentences))\n",
      "    print '%-32s' % 'longest sentence (letters)', '%-16s' % len(get_longest_sentence_by_characters(sentences))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Tokenizer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create word tokens from sentences\n",
      "* pattern1: no punctuation\n",
      "* pattern2: include punctuations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "english_stopwords = stopwords.words('english')\n",
      "\n",
      "def tokenize_text(text, alnum_only = False,  alpha_only = False,remove_stopwords=False, use_pattern = 1):\n",
      "    pattern1 = [\"\\w+[\\-|']\\w+\", #words joined with '-' or words with ' in them \\\n",
      "               \"(\\w+/)+\\w+\", #document references and words that are seperated with '\\' \\\n",
      "               \"\\([\\w+\\s*]\\)\", #words with parenthesis in them. this will exclude parenthisis \\\n",
      "               \"\\w+\"]\n",
      "\n",
      "    pattern2 =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-']\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern  = pattern1 if use_pattern ==1 else pattern2\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list)== True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    tokens =  [token for token in tokens \\\n",
      "                  if ((remove_stopwords and token.lower() not in english_stopwords) or not remove_stopwords) \\\n",
      "                  and ((alnum_only and token.isalnum()) or not alnum_only)\n",
      "                  and ((alpha_only and token.isalpha()) or not alpha_only)]\n",
      "    return tokens\n",
      "# tokenize words but keep sentences seperate\n",
      "def tokenize_sentence_text(sentences, alnum_only = False, alpha_only = False, remove_stopwords=False, use_pattern = 1, show_pbar=show_pbars):\n",
      "    sent_tokens = []\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Tokenizing sentences\", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(sentences)).start()\n",
      "    i = 0\n",
      "    for sent in sentences:\n",
      "        sent_token = tokenize_text(sent, alnum_only=alnum_only, alpha_only = alpha_only, remove_stopwords=remove_stopwords, use_pattern=use_pattern)\n",
      "        sent_tokens.append(sent_token)\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return sent_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_word_count(tokens):\n",
      "    return len(tokens)\n",
      "\n",
      "def get_unique_word_count(tokens):\n",
      "    return len(set(tokens))\n",
      "\n",
      "def get_average_word_length(tokens):\n",
      "    return sum([len(token) for token in tokens])/float(len(tokens))\n",
      "\n",
      "def get_longest_word(tokens, ignore_join_words = True):\n",
      "    if ignore_join_words:\n",
      "        return \", \".join([max([token for token in tokens if token.isalpha()], key=lambda token:len(token))])\n",
      "    else:\n",
      "        return \", \".join([max([token for token in tokens], key=lambda token:len(token))])\n",
      "def print_word_stats(tokens):\n",
      "    print '%-32s' % 'Info type', '%-16s' % 'Value'\n",
      "    print '%-32s' % 'number of words', '%-16d' % get_word_count(tokens)\n",
      "    print '%-32s' % 'number of unique words', '%-16d' % get_unique_word_count(tokens)\n",
      "    print '%-32s' % 'average word length', '%-16.2f' % get_average_word_length(tokens)\n",
      "    #exclude joint words for longest word\n",
      "    print '%-32s' % 'longest single word', '%-16s' % get_longest_word(tokens, ignore_join_words = True)\n",
      "    print '%-32s' % 'longest word', '%-16s' % get_longest_word(tokens, ignore_join_words = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part of Speech Taggers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "This is a manually tagged tagger of cities and countries from the gazetteers corpus. I also appended a special tag or United Nations because the tagger kept detecting United as a verb. \n",
      "\n",
      "I am using custom tags (NPLOC and NPORG) because I don't want them to be mixed with other NPs while having the chunker detect them as a sub class of NP. United Nations was especially important because it occurs alot and is United is flagged as a verb which throws off the chunker, especially the verb object chunker.\n",
      "\n",
      "I experimented with The regex tagger only support 100 groups max and the it won't deal with tokenized sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#location/organization tagger\n",
      "def get_location_tagger_tags():\n",
      "    from nltk.corpus import gazetteers as gz\n",
      "    pos_tags_locations =[[('United', 'NPORG'), ('Nations', 'NPORG')],\n",
      "                         [('Working', 'NPORG'), ('Party', 'NPORG')]]\n",
      "    pos_tags_locations +=[[(word, 'NPLOC') for word in words.split(' ') if word not in english_stopwords] for words in gz.words()]\n",
      "    '''\n",
      "#   pos_tags_locations += [[(words, 'NPLOC') ]for words in gz.words() ]\n",
      "#   pos_tags_locations = [(words, 'NPLOC') for words in [gzwords for gzwords in gz.words() if len(gzwords.split(' '))>2]][:90]\n",
      "    pos_tags_locations+=[(r'\\w* Republic \\w*', 'NPLOC'),(r'\\w* Kingdom \\w*', 'NPLOC'),('United Nations', 'NPORG'),('Working Party', 'NPORG')]\n",
      "    '''\n",
      "    \n",
      "    return pos_tags_locations\n",
      "\n",
      "#the tagger here can be a 3 stage tagger or 5 state if the location tagger is included\n",
      "#note that the location tagger is used first to ensure the location/orgainzations are detected properly\n",
      "def build_backoff_tagger (train_sents, default='NN', include_location_tagger=False):\n",
      "    t0 = nltk.DefaultTagger(default)\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    t4=t2\n",
      "    if include_location_tagger:\n",
      "        t3 = nltk.RegexpTagger([(r'UN[A-Z]*', 'NPORG')], backoff=t2)\n",
      "        t4 = nltk.UnigramTagger(get_location_tagger_tags(), backoff=t3) #tag using the custom tagger first if requested\n",
      "    return t4\n",
      "\n",
      "def build_location_tagger():\n",
      "    t0 = nltk.DefaultTagger('U')\n",
      "    return nltk.BigramTagger(get_location_tagger_tags(), backoff=t0)\n",
      "\n",
      "def get_brown_tagger(category = None, include_location_tagger=False):\n",
      "    if category:\n",
      "        return build_backoff_tagger(brown.tagged_sents(categories=category), include_location_tagger=include_location_tagger)\n",
      "    else:\n",
      "        return build_backoff_tagger(brown.tagged_sents(), include_location_tagger=include_location_tagger)\n",
      "    \n",
      "def get_default_treebank_tagger():\n",
      "    return nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n",
      "\n",
      "#Tag the sentences based on the selected tagger\n",
      "\n",
      "def tag_pos_sentences(tokenized_sentences, tagger=get_default_treebank_tagger(), show_pbar=show_pbars):\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"tagging sentences\", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(tokenized_sentences)).start()\n",
      "    i = 0\n",
      "    tagged_sentences = []\n",
      "    for sent in tokenized_sentences:\n",
      "        tagged_sentences.append(tagger.tag(sent))\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return tagged_sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text Chunker"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My pride an joy chunker tries to do alot. I experimented heavily with it in previous assignments to ensure it captures complex objects. I am targeting 2 main classes:\n",
      "* **PNS**: Proper nouns which in this case can be as long as 7 words for some UN organizations\n",
      "* **VNS**: Verb noun subjects (or who did what)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_punctuation(text):\n",
      "    return \"\".join(c for c in text if c not in string.punctuation)\n",
      "#it assumes brown tag set and proper nouns as default parameters. I found brown to be best in detecting proper nouns\n",
      "def get_chunker(grammer=None, tag_set=None, target='PNS'):\n",
      "    if not grammer:\n",
      "        #not used here\n",
      "        if tag_set=='treebank':\n",
      "            grammer = r\"\"\"PNS: {<DT|JJ.*|N.*>+<IN>*<DT|J.*|N.*>*<NNP.*><CD>*}\n",
      "                          {<DT|J.*|N.*>*<NNP.*><CD>*}\n",
      "                            \"\"\"\n",
      "        elif tag_set is None or tag_set=='brown':\n",
      "            if target=='PNS':\n",
      "                #words that precede or folllow proper nouns that are part of it (e.g. UN commission for xxx)\n",
      "                DJNP = \"<DT|J.*|FW-J.*|VBG.*|VBN.*\"\\\n",
      "                    +\"|N.*|FW-N.*\"\\\n",
      "                    +\"|NP.*|FW-NP.*>\"\n",
      "                #proper nouns\n",
      "                NPS = \"<NP.*|FW-NP.*>\" \n",
      "\n",
      "                grammer = r\"PNS: {\"+DJNP+\"+<IN.*>*\"+DJNP+\"*\"+NPS+\"<CD|MD>*}\"+\"\\n{\"+DJNP+\"*\"+NPS+\"<CD|CD-TL|MD|N.*>*}\"\n",
      "            \n",
      "            elif target=='VNS':\n",
      "                #verbs and teh different nouns that come before it or after it\n",
      "                grammer  = r\"\"\"VNS: {<N.*|J.*>*<VB.*>+<CD|TO|IN|CC|DT|PRP>*<DT|J.*|N.*>*<N.*|J.*>}\n",
      "                        {<N.*|J.*>+<CD|TO|IN|CC|DT|PRP>*<DT|J.*|N.*>*<VB.*>+}\n",
      "                         \"\"\"\n",
      "        #not used here\n",
      "        elif tag_set=='brown_simple':\n",
      "            grammer = r\"\"\"PNS: {<DET|ADJ|N>*<NP><NUM>*}\n",
      "                            {<DET|ADJ|N>+<IN>*<DET|ADJ|N><NP><NUM>*}\"\"\"\n",
      "    return nltk.RegexpParser(grammer)\n",
      "# returns chunk trees based on teh chunker and target chosesn\n",
      "# randomize and limit are for testing purposues. They allow the chunker to run on a limited number of random sentences\n",
      "def get_chunks(tagged_sents, chunker=get_chunker(), target = 'PNS', limit=0, randomize= False, show_pbar=show_pbars):\n",
      "    chunks = []\n",
      "    limit = limit if limit>0 else len(tagged_sents)\n",
      "    i=0\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Chunking \", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=limit).start()\n",
      "    import random\n",
      "    for index in range(limit):\n",
      "        sent_index = random.randint(0, len(tagged_sents)-1) if randomize else index\n",
      "        sent = tagged_sents[sent_index]\n",
      "        if len(sent)>0:\n",
      "            result = chunker.parse(sent) \n",
      "            chunks.append(result)\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return chunks\n",
      "#extracts the target chunks from the tree. I seperated this because in some cases I need the trees\n",
      "def extract_target_from_chunks(raw_chunks, target='PNS',print_output=False, print_leaves = False):\n",
      "    chunks=[]\n",
      "    for result in raw_chunks:\n",
      "        phrase_leaves = [subtree.leaves() for subtree in  result.subtrees() if subtree.node==target]\n",
      "        if print_leaves:\n",
      "            print phrase_leaves\n",
      "        words = [\" \".join([word for (word,tag) in leaf]) for leaf in phrase_leaves ]\n",
      "        if len(words)>0:\n",
      "            chunks.append(words)\n",
      "        if print_output:\n",
      "            print \"\\t\".join([word for (word, tag) in sent])\n",
      "            print \"\\t\".join([tag for (word, tag) in sent])\n",
      "            print \"\\n\".join(['%d\\t%s'%(j+1, words[j]) for j in range(len(words))])\n",
      "            print '\\n'\n",
      "        \n",
      "    return [phrase for phrases in chunks for phrase in phrases]\n",
      "#flattens the trees but doesn't remove anything. It only groups the target one tuple. \n",
      "# This helps generate collocations based on complex chunker grammer which was very useful\n",
      "def flatten_chunks(chunks, target='PNS'):\n",
      "    flat_chunks = []\n",
      "    for chunk in chunks:\n",
      "        flat_chunk=[]\n",
      "        for n in chunk:\n",
      "            if isinstance(n, tuple):\n",
      "                flat_chunk.append(n)\n",
      "            elif isinstance(n, nltk.tree.Tree):\n",
      "                flat_node = (\" \".join([word for (word, tag) in n.leaves()]), n.node)\n",
      "                flat_chunk.append(flat_node)\n",
      "        flat_chunks.append(flat_chunk)\n",
      "    return flat_chunks\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Process Chunks to generate chunk Frequency Distrubtions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Named Entities using a multi stage chunker\n",
      "* Verb objects"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_chunks(sentences=None, sent_tokens=None, tagged_sentences = None,  remove_months = True, tagger = None):\n",
      "    if not tagged_sentences:\n",
      "        sent_tokens = sent_tokens if sent_tokens else \\\n",
      "            tokenize_sentence_text(sentences, alnum_only=False, remove_stopwords=False, use_pattern = 2)\n",
      "        tagger = get_brown_tagger(include_location_tagger=True)\n",
      "    tagged_sentences = tagged_sentences if tagged_sentences else tag_pos_sentences(sent_tokens, tagger=tagger, show_pbar=show_pbars)\n",
      "    #get the proper noun chunks from teh chunker\n",
      "    nchunks = get_chunks(tagged_sentences, chunker=get_chunker(tag_set='brown', target='PNS'), target = 'PNS', show_pbar=show_pbars)\n",
      "    nchunks = extract_target_from_chunks(nchunks, target='PNS')\n",
      "    # there are many month names mentioned in the FrewDist, I am removing them because I don' think they add much value\n",
      "    if remove_months:\n",
      "        import calendar\n",
      "        nchunks = [chunk for chunk in nchunks if len(remove_punctuation(chunk))>1 and chunk not in calendar.month_name]\n",
      "    \n",
      "    vchunks = get_chunks(tagged_sentences, chunker=get_chunker(tag_set='brown', target='VNS'), target = 'VNS', show_pbar=show_pbars)\n",
      "    vchunks = extract_target_from_chunks(vchunks, target='VNS')\n",
      "#     vchunks = [chunk for chunk in vchunks if chunk not in nchunks]\n",
      "    nchunks_fd = nltk.FreqDist(nchunks)\n",
      "    vchunks_fd = nltk.FreqDist(vchunks)\n",
      "    return print_FreqDists([nchunks_fd, vchunks_fd], titles=['Proper Nouns', 'Verb Objects'], csv=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Collocations\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two collocation implementations here:\n",
      "* word based: collocation of individual words\n",
      "* chunk based: collocation using a whole chunker grammer element (e.g. PNS). This will treat the whole element as one words and generate collocation of other words with it. this helps especially when dealing with multi-words country or organization names\n",
      "\n",
      "The output is four finders nbests:\n",
      "(bigram, trigram) x (pmi, chi_sq)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.collocations import *\n",
      "#find pure word frequency collocations\n",
      "#it doesn't matter whether I pass tagged sentences or not since I am focusing on words only\n",
      "def get_collocations(sentences=None, sent_tokens=None, filter_limit = 3, finder_limit = 20):\n",
      "#     from nltk.collocations import *\n",
      "    sent_tokens = sent_tokens if sent_tokens else tokenize_sentence_text(sentences,alpha_only = True,\\\n",
      "                                                                         remove_stopwords=True, use_pattern = 1, show_pbar=show_pbars)\n",
      "    word_tokens = [word for sent in sent_tokens for word in sent]\n",
      "#     print word_tokens[:5]\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    finder = BigramCollocationFinder.from_words(word_tokens)\n",
      "    finder3 = TrigramCollocationFinder.from_words(word_tokens)\n",
      "    finder.apply_freq_filter(filter_limit)\n",
      "    finder3.apply_freq_filter(filter_limit)\n",
      "    #print 4 finders (bigram, trigram) x (pmi, chi_sq)\n",
      "    f1= finder.nbest(bigram_measures.pmi, finder_limit)\n",
      "    f2= finder.nbest(bigram_measures.chi_sq, finder_limit)\n",
      "    f3= finder3.nbest(trigram_measures.pmi, finder_limit)\n",
      "    f4= finder3.nbest(trigram_measures.chi_sq, finder_limit)\n",
      "    return f1, f2,f3,f4\n",
      "\n",
      "#get the colloations based on chunks    \n",
      "def get_chunked_collocations(sentences=None,tagged_sentences=None, tagger=None, \\\n",
      "                             target='PNS', chunker = None, filter_limit = 3, finder_limit = 20):\n",
      "\n",
      "    #I can pass pre-tagged sentences if needed\n",
      "    if tagger is None:\n",
      "        get_brown_tagger(include_location_tagger=True)\n",
      "    if sentences:\n",
      "        sent_tokens = tokenize_sentence_text(sentences, alnum_only=False, remove_stopwords=False, use_pattern = 2)\n",
      "        tagged_sentences = tag_pos_sentences(sent_tokens, tagger=tagger, show_pbar=show_pbars)\n",
      "    \n",
      "    chunker = chunker if chunker else get_chunker(tag_set='brown', target = target)\n",
      "    chunks = get_chunks(tagged_sentences,chunker=chunker, target = target,show_pbar=show_pbars )\n",
      "    flat_chunks = flatten_chunks(chunks, target=target)\n",
      "    flat_chunk_tokens = [token for flat_chunk in flat_chunks for token in flat_chunk]\n",
      "    \n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    finder = BigramCollocationFinder.from_words(flat_chunk_tokens)\n",
      "    finder3 = TrigramCollocationFinder.from_words(flat_chunk_tokens)\n",
      "    \n",
      "    #after some testing, I found that many collocations are numeric so I am filtering out non alpha words\n",
      "    #This filters out any pairs that don't included the target chunk grammer\n",
      "    finder.apply_ngram_filter(lambda (w1, t1), (w2,t2): target not in (t1,t2) or not w1.isalpha() or not w2.isalpha())\n",
      "    finder3.apply_ngram_filter(lambda (w1, t1), (w2,t2), (w3, t3): \\\n",
      "                               target not in (t1,t2,t3) or not w1.isalpha() or not w2.isalpha() or not w3.isalpha())\n",
      "    finder.apply_freq_filter(filter_limit)\n",
      "    finder3.apply_freq_filter(filter_limit)\n",
      "    #print 4 finders (bigram, trigram) x (pmi, chi_sq)\n",
      "    f1= finder.nbest(bigram_measures.pmi, finder_limit)\n",
      "    f2= finder.nbest(bigram_measures.chi_sq, finder_limit)\n",
      "    f3= finder3.nbest(trigram_measures.pmi, finder_limit)\n",
      "    f4= finder3.nbest(trigram_measures.chi_sq, finder_limit)\n",
      "    return f1, f2,f3,f4\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequent Terms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These modules help generate an ngram frequncy distribution. It takes n as an input to plut any number ngrams.\n",
      "\n",
      "It also takes other options about the text (e.g. remove stopwords or lower case everything). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer\n",
      "snowball_stemmer = SnowballStemmer(\"english\")\n",
      "\n",
      "\n",
      "def get_normalized_word(token,stem_words=False, lower_case=False):\n",
      "    if stem_words and len(token)>1:\n",
      "        try:\n",
      "            return snowball_stemmer.stem(token)\n",
      "        except Exception, e:\n",
      "            print token, e\n",
      "            return token\n",
      "    elif lower_case:\n",
      "        return token.lower()\n",
      "    else:\n",
      "        return token\n",
      "    #I wanted to keep CAP words CAPs because the mostly reflect abbreviations but some common words appear as all caps in headers\n",
      "    '''if token.isupper(): \n",
      "        return token\n",
      "    else:\n",
      "        return token.lower()\n",
      "        '''\n",
      "   \n",
      "    \n",
      "def get_normalzed_ngram(ngram, stem_words=False, lower_case=False):\n",
      "    return [get_normalized_word(word, stem_words, lower_case) for word in ngram]\n",
      "\n",
      "#it can take either tokeinzed or text sentences. the output is a frequency distribution of ngrams\n",
      "def get_frequent_ngrams(sentences=None, sent_tokens=None, ngram_length = 1, alnum_only = False, remove_stopwords=False, stem_words = False, lower_case=False):\n",
      "    sent_tokens = sent_tokens if sent_tokens else tokenize_sentence_text(sentences, alnum_only=alnum_only, remove_stopwords=remove_stopwords, use_pattern = 2)\n",
      "    ngrams = [\" \".join(get_normalzed_ngram(ngram, stem_words, lower_case))  \\\n",
      "                       for sent in sent_tokens for ngram in nltk.ngrams(sent, ngram_length) ]\n",
      "    fd_ngrams = nltk.FreqDist(ngrams)\n",
      "    return fd_ngrams\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Processing NGrams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generates 5 different frequency distributions:\n",
      "* Unigrams (not very useful)\n",
      "* Unigrams with stemming (not very useful)\n",
      "* Bigrams (n=2)\n",
      "* Trigrams (n=3)\n",
      "* Quadgrams (n=4) \n",
      "\n",
      "All ngrams are alpha numeric, lowercase, and without stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_ngrams(sentences=None, sent_tokens=None, limit = 50):\n",
      "    sent_tokens = sent_tokens if sent_tokens else tokenize_sentence_text(sentences, alnum_only=True, \\\n",
      "                                                                          remove_stopwords=True, use_pattern = 2)\n",
      "    unigrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens, ngram_length = 1, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    unigrams_stem_fd = get_frequent_ngrams(sent_tokens=sent_tokens,ngram_length =  1, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True, stem_words=True)\n",
      "    bigrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens, ngram_length = 2, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    trigrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens, ngram_length = 3, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    quadgrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens,ngram_length =  4, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    return print_FreqDists([unigrams_fd,unigrams_stem_fd, bigrams_fd,trigrams_fd, quadgrams_fd],\\\n",
      "                           titles=['Unigram', 'Stemed Unigram', 'Bigrams', 'Trigrams', 'Quadgrams'], limit=limit, csv=True)\n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extract Document Links"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extracts document references from text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DOCUMENT_LINK_PATTERN = '([A-Z0-9._-]+/)+([A-z0-9._-]+)*'\n",
      "def extract_links_from_documents(docs, show_pbar=show_pbars):\n",
      "    links = {}\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Extracting Outgoing Links \", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(docs)).start()\n",
      "    i = 0\n",
      "    doc_ids = dict([(docs[doc]['id'], doc) for doc in docs])\n",
      "    for doc in docs:\n",
      "        links[doc] ={'outgoing':[], 'incoming':[]}\n",
      "    for doc in docs:\n",
      "        olinks= extract_links_from_document(docs[doc])\n",
      "        for olink in olinks:\n",
      "            if olink in doc_ids:\n",
      "                links[doc_ids[olink]]['incoming'].append(docs[doc]['id'])\n",
      "        links[doc]['outgoing']=olinks\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return links\n",
      "def extract_links_from_document(doc):\n",
      "    text = \"\\n\".join([ \" \".join(c) for c in doc['content']])\n",
      "    links = re.finditer(DOCUMENT_LINK_PATTERN, text)\n",
      "    result = [link.group(0) for link in links]\n",
      "    return result\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Printing Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These functions help print outputs nicely (e.g. multiple frequency distributions side by side in a table)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def print_FreqDist(fd, limit =0):\n",
      "    if limit == 0:\n",
      "        limit = len(fd.items())\n",
      "    print \"\\n\".join([\"%d\\t%s\"%( value, word) for (word, value) in fd.items()[:limit]])\n",
      "    \n",
      "#prints multiple frequency distribution next to each other to compare results.\n",
      "def print_FreqDists(fds, titles=None, limit = 50, csv=False):\n",
      "    return print_csv_table(preprint_FreqDists(fds, titles, limit, csv=True))\n",
      "\n",
      "def preprint_FreqDists(fds, titles=None, limit = 50, csv=False):\n",
      "    lines = ''\n",
      "    html_str =''\n",
      "    if limit ==0:\n",
      "        limit = max([len(fd) for fd in fds])\n",
      "    titles = titles if titles else [i for i in range(len(fds))]\n",
      "    lines+=\",\".join(['%s phrase,%s frequency'%(t,t) for t in titles])+'\\n'\n",
      "    for i in range(limit):\n",
      "        line = '%d'%(i+1)\n",
      "        for fd in fds:\n",
      "            key = ''\n",
      "            val = 0\n",
      "            if i < len(fd.items()):\n",
      "                key =fd.items()[i][0]\n",
      "                val = fd.items()[i][1]\n",
      "            if csv:\n",
      "                line='%s,\"%s\",%d'%(line,key,val)\n",
      "            else:\n",
      "                line='%s\\t%s\\t%d'%(line,key,val)\n",
      "       \n",
      "#         print line\n",
      "        lines+='%s\\n'%line\n",
      "    return lines\n",
      "\n",
      "def print_csv_table(csv_lines):\n",
      "    import pandas, io\n",
      "    if isinstance(csv_lines, list):\n",
      "        csv_lines = \"\\n\".join(csv_lines)\n",
      "    \n",
      "    plines= pandas.read_csv(io.BytesIO(str(csv_lines)))\n",
      "    return plines\n",
      "    \n",
      "def print_pos_tagged_sentences(tagged_sentences):\n",
      "    formatted_sents = []\n",
      "    \n",
      "    import io\n",
      "    max_tokens = max([len(sent) for sent in tagged_sentences])\n",
      "    formatted_sents.append(\",\".join([' ' for i in range(max_tokens)]))\n",
      "    for sent in tagged_sentences:\n",
      "        words = ['\"%s\"'%word for (word, tag) in sent]\n",
      "        tags = ['\"%s\"'%tag for (word, tag) in sent]\n",
      "        if len(words)<max_tokens:\n",
      "            words+=[' ' for i in range(max_tokens - len(words))]\n",
      "            tags+=[' ' for i in range(max_tokens - len(tags))]\n",
      "        csv_line = \"%s\\n%s\\n%s\\n\"%(\",\".join(words), \",\".join(tags), \",\".join([' ' for i in range(max_tokens)]))\n",
      "        formatted_sents.append(','.join(words))\n",
      "        formatted_sents.append(','.join(tags))\n",
      "        formatted_sents.append( \",\".join([' ' for i in range(max_tokens)]))\n",
      "    return formatted_sents\n",
      "\n",
      "#allow me to limit the number of chars in the output whithout whitespace. I am using a table output that generates tons of white space\n",
      "def shrink_output_text(text, char_limit=2500, count_whitespace = False):\n",
      "    text2 = [(i, str(text[i])) for i in range(len(text)) if text[i]!=' ']\n",
      "    if count_whitespace:\n",
      "        return text[:char_limit]\n",
      "    else:\n",
      "        try:\n",
      "            last_char_tuple = text2[len(text2)-1][0] if len(text2)<=char_limit else text2[char_limit][0]\n",
      "            return text[:last_char_tuple]\n",
      "        except Exception, e:\n",
      "            print e\n",
      "            print len(text2), char_limit\n",
      "            return text[:char_limit]\n",
      "        \n",
      "def print_collocations_finders(finders, chunked=False):\n",
      "    finders = list(finders)\n",
      "    if chunked:\n",
      "        \n",
      "        for i in range(len(finders)):\n",
      "#             print finders[i][:3]\n",
      "            finders[i] = [tuple([word_tag[0] for word_tag in item]) for item in finders[i] ]\n",
      "#             print finders[i][:3]\n",
      "    output = []\n",
      "    total_width = sum([len(finder[0]) for finder in finders])\n",
      "#     print total_width\n",
      "#     output.append(','.join([' ' for i in range(total_width + len(finders))]))\n",
      "    output.append('bigram pmi, , ,bigram chi_sq, , ,trigram pmi, , , ,tirgram chi_sq, , ' )\n",
      "    for i in range(len(finders[0])):\n",
      "        line = \" ,\"\n",
      "        for finder in finders:\n",
      "            if i< len(finder):\n",
      "                line+=\",\".join(finder[i])\n",
      "            line+=\",|,\"\n",
      "        line= line[:-4]\n",
      "        output.append(line)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate HTML"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "\n",
      "IGNORE_LIST = ['jobs', 'Display PDF File', 'links', 'Download File' ]\n",
      "def is_heading(para):\n",
      "    return len(para)==1 and para[0][-1] not in ['.', ':', '\"'] and para[0][0] not in ['*', '(', '\"']\n",
      "\n",
      "\n",
      "def json2html(obj, ignore_list = IGNORE_LIST):\n",
      "    html = '<table>'\n",
      "    isDict = isinstance(obj, dict)\n",
      "    i = 0\n",
      "    for o in obj:\n",
      "        key = o if isDict else i+1\n",
      "#         print type(obj[key])\n",
      "        value = obj[o] if isDict else o\n",
      "        if isinstance(value, dict):\n",
      "            value = json2html(value, ignore_list)\n",
      "        elif isinstance(value, list):\n",
      "            value =  json2html(value, ignore_list)\n",
      "        \n",
      "        if key =='link':\n",
      "            value = '<a href=\"%s\" target=\"_blank\">%s</a>'%(value, value)\n",
      "#         value = obj[o] if type(o) is str or type(o) is unicode \\\n",
      "#                 else \",\".join(obj[o]) if type(o) is list \\\n",
      "#                 else json2html(obj[o])\n",
      "        if key not in ignore_list:\n",
      "            html+='<tr><td>%s</td><td>%s</td></tr>'%(key, value)\n",
      "        i+=1\n",
      "    html+='</table>'\n",
      "    return html\n",
      "\n",
      "\n",
      "def get_doc_html(doc):\n",
      "    html = ''\n",
      "    for para in doc['contens']:\n",
      "        if len(para) ==1:\n",
      "            html+='<h1>%s</h1>'%para[0]\n",
      "        else:\n",
      "            html+= '<p>%s</p>'%(\" \".join(para))\n",
      "            \n",
      "#     html = [ '<p>%s</p>'%(\" \".join(para)) for para in doc['content']]\n",
      "    return html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    }
   ],
   "metadata": {}
  }
 ]
}