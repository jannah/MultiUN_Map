{
 "metadata": {
  "gist_id": "6726db02bf6bdfbc209a",
  "name": "",
  "signature": "sha256:38935245cf49f012eac17639f44df559679632a234efc5fa130a8c851c8348bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Multi United Nations Corpus Sever Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a list of all the service functions used to access and process the Multi UN corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "import os\n",
      "import chardet\n",
      "from unidecode import unidecode\n",
      "from progressbar import AnimatedMarker, Bar, BouncingBar,\\\n",
      "                            Counter, ETA, Percentage, ProgressBar, SimpleProgress, FileTransferSpeed\n",
      "\n",
      "PATH_TO_FILES = \"C:\\\\Users\\\\Hassan\\\\Documents\\\\iSchool\\\\NLP\\\\United Nations\\\\multiUN.en\\\\un\\\\txt\\\\en\"  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_files(year = None, raw=True):\n",
      "    years = []\n",
      "    if year is None:\n",
      "        years = [ year for year in os.listdir(PATH_TO_FILES) if not '.txt' in year or '_OLD_' in year]\n",
      "    else:\n",
      "        years = [str(year)]\n",
      "    data = {}\n",
      "    pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(years)).start()\n",
      "    for i in range(len(years)):\n",
      "        y = years[i]\n",
      "        data[y] = load_files_by_year(y, raw)\n",
      "        pbar.update(i+1)\n",
      "    pbar.finish()\n",
      "    return data\n",
      "\n",
      "\n",
      "def load_files_by_year(year, raw=True):\n",
      "    texts = []\n",
      "    file_type = 'raw' if raw else 'txt'\n",
      "    full_path = os.path.join(PATH_TO_FILES, year, file_type)        \n",
      "    files = os.listdir(full_path)\n",
      "    pbar2 = ProgressBar(widgets=[year, SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(files)).start()\n",
      "    for i in range(len(files)):\n",
      "        filename = os.path.join(full_path, files[i])\n",
      "        with open(filename, 'r') as f:\n",
      "            text = f.read()\n",
      "            text = text.decode('utf-8') #the regular text was throwing an exception complaining about ascii\n",
      "            texts.append(text) #Keeping each file in a seperate array element\n",
      "            pbar2.update(i+1)\n",
      "    pbar2.finish()\n",
      "    return texts    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 178
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fix Unicode and Incomplete Sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fix_unicode(s):\n",
      "    text = ''\n",
      "    try:\n",
      "        text = str(s)\n",
      "        return text\n",
      "    except:\n",
      "        try:\n",
      "#             print 'encoding:', chardet.detect(s)['encoding']\n",
      "            text = s.encode(chardet.detect(s)['encoding'])\n",
      "            return text\n",
      "        except Exception, e:\n",
      "#             print e\n",
      "            try:\n",
      "                text = unidecode(s)\n",
      "                return text\n",
      "#                 f.write(sent)\n",
      "            except Exception, e2:\n",
      "#                 print 'unidecode:', e2\n",
      "                print 'error: ', e2\n",
      "                print s\n",
      "                return text\n",
      "                pass\n",
      "INCOMPLETE_SUFFIXES = ['Mr.', 'Ms.', 'Ch.']\n",
      "def fix_incomplete_sentences(para):\n",
      "    \n",
      "    sents = []\n",
      "    for i in range(len(para)):\n",
      "        sent = para[i]\n",
      "        out_sent=para[i]\n",
      "        while any(sent.endswith(suffix) for suffix in INCOMPLETE_SUFFIXES) and i<(len(para)-1):\n",
      "            i+=1\n",
      "            out_sent+=' %s'%(para[i].strip())\n",
      "            sent=para[i]\n",
      "        sents.append(out_sent)\n",
      "    return sents\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 179
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Load XML Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PATH_TO_XML_FILES=\"C:\\\\Users\\\\Hassan\\\\Documents\\\\iSchool\\\\NLP\\\\United Nations\\\\multiUN.en\\\\un\\\\xml\\\\en\"\n",
      "from lxml import etree\n",
      "#data ={}\n",
      "def load_xml_files(year = None, path = PATH_TO_XML_FILES, show_pbar=True):\n",
      "    data = {}\n",
      "    years = []\n",
      "    if year is None:\n",
      "        years = [ year for year in os.listdir(path)]\n",
      "    else:\n",
      "        years = [str(year)]\n",
      "    \n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(years)).start()\n",
      "    for i in range(len(years)):\n",
      "        y = years[i]\n",
      "        data[y] = load_xml_files_by_year(y, path, show_pbar)\n",
      "        if show_pbar:\n",
      "            pbar.update(i+1)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return data\n",
      "\n",
      "def load_xml_files_by_year(year, path = PATH_TO_XML_FILES, show_pbar = True):\n",
      "    documents = {}\n",
      "    full_path = os.path.join(path, year)    \n",
      "#     print full_path\n",
      "    files = [fname for fname in os.listdir(full_path) if fname.endswith('.xml')]\n",
      "    if show_pbar:\n",
      "        pbar2 = ProgressBar(widgets=[year, SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(files)).start()\n",
      "    for i in range(len(files)):\n",
      "        f = files[i]\n",
      "        filename = os.path.join(full_path, f)\n",
      "        documents[f]=[]\n",
      "        #with open(filename, 'r') as f:\n",
      "        tree = etree.parse(filename)\n",
      "        root = tree.getroot()\n",
      "        xparas =  root.getchildren()[0].getchildren()[0].getchildren()\n",
      "        for xpara in xparas:\n",
      "           documents[f].append([fix_unicode(sent.text.strip()) for sent in xpara if len(sent.text.strip())>0])\n",
      "        #text = f.read()\n",
      "        #text = text.decode('utf-8') #the regular text was throwing an exception complaining about ascii\n",
      "        #texts.append(text) #Keeping each file in a seperate array element\n",
      "        if show_pbar:\n",
      "            pbar2.update(i+1)\n",
      "    if show_pbar:\n",
      "        pbar2.finish()\n",
      "    return documents\n",
      "\n",
      "\n",
      "\n",
      "def flatten_document_structure_paragraphs(doc_dict):\n",
      "    print 'flattening paragraphs'\n",
      "    flat = [fix_incomplete_sentences(para) for year in doc_dict for doc in doc_dict[year] for para in doc_dict[year][doc]]\n",
      "    return flat\n",
      "def flatten_document_structure(doc_dict):\n",
      "    print 'flattening'\n",
      "    text = [fix_incomplete_sentences(para) for year in doc_dict for doc in doc_dict[year] for para in doc_dict[year][doc]]\n",
      "    text = [sent for para in text for sent in para]\n",
      "        \n",
      "    return text\n",
      "   \n",
      "\n",
      "def flatten_document_structure_long(doc_dict):\n",
      "    return flatten_document_structure(doc_dict)\n",
      "#     text = [ sent for year in doc_dict for doc in doc_dict[year] for para in doc_dict[year][doc] for sent in para]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentence Tokenizers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def tokenize_sentences(text, use_nltk_tokenizer = False ):\n",
      "    sents = []\n",
      "    if use_nltk_tokenizer:\n",
      "\n",
      "        sents = sent_tokenizer.sentences_from_text(text)\n",
      "    else:\n",
      "        #sents = \"\\n\".join(texts) # join all the documents into one big string\n",
      "        #split the document by \\n and remove any empty line and extra whitespace\n",
      "        sents = [sent.strip() for sent in text.split('\\n') if len(sent.strip())>0] \n",
      "    return sents\n",
      "def tokenize_sentences2(text, use_nltk_tokenizer = False ):\n",
      "    sents = []\n",
      "    global sent_tokenizer\n",
      "    if use_nltk_tokenizer:\n",
      "        sents = sent_tokenizer.sentences_from_text(text)\n",
      "    else:\n",
      "        #sents = \"\\n\".join(texts) # join all the documents into one big string\n",
      "        #split the document by \\n and remove any empty line and extra whitespace\n",
      "        paragraphs = [sent.strip() for sent in text.split('\\n') if len(sent.strip())>0] \n",
      "        #some sentence boundaries are wrong. Some text is split on numbered lists and other abbreviations. hence the join\n",
      "        sents = [\" \".join(sent) for sent in [sent_tokenizer.sentences_from_text(para) for para in paragraphs]]\n",
      "    return sents\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 181
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sentence Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentence_count(sentences):\n",
      "    return len(sentences)\n",
      "\n",
      "def get_average_words_per_sentence(sentences):\n",
      "    \n",
      "    return sum([len(sent.split(' ')) for sent in sentences])/float(len(sentences))\n",
      "\n",
      "def get_average_characters_per_sentence(sentences):\n",
      "    return sum([len(sent) for sent in sentences])/float(len(sentences))\n",
      "\n",
      "def get_longest_sentence_by_words(sentences):\n",
      "    return max(sentences, key=lambda w:len(w)).split(\" \")\n",
      "\n",
      "def get_longest_sentence_by_characters(sentences):\n",
      "    return max(sentences, key=lambda w:len(w))\n",
      "\n",
      "def print_sentence_statistics(sentences):\n",
      "    print '%-32s' % 'Info type', '%-16s' % 'Value',  '\\n----------------------------------------'\n",
      "    print '%-32s' % 'number of sentences', '%-16d' % len(sentences)\n",
      "    print '%-32s' % 'average sentence length(words)', '%-16.2f' % get_average_words_per_sentence(sentences)\n",
      "    print '%-32s' % 'average sentence length(letters)', '%-16.2f' % get_average_characters_per_sentence(sentences)\n",
      "    print '%-32s' % 'longest sentence (words)', '%-16s' % len(get_longest_sentence_by_words(sentences))\n",
      "    print '%-32s' % 'longest sentence (letters)', '%-16s' % len(get_longest_sentence_by_characters(sentences))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 182
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Tokenizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "english_stopwords = stopwords.words('english')\n",
      "\n",
      "def tokenize_text(text, alnum_only = False,  alpha_only = False,remove_stopwords=False, use_pattern = 1):\n",
      "    pattern1 = [\"\\w+[\\-|']\\w+\", #words joined with '-' or words with ' in them \\\n",
      "               \"(\\w+/)+\\w+\", #document references and words that are seperated with '\\' \\\n",
      "               \"\\([\\w+\\s*]\\)\", #words with parenthesis in them. this will exclude parenthisis \\\n",
      "               \"\\w+\"]\n",
      "\n",
      "    pattern2 =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-']\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern  = pattern1 if use_pattern ==1 else pattern2\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list)== True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    tokens =  [token for token in tokens \\\n",
      "                  if ((remove_stopwords and token.lower() not in english_stopwords) or not remove_stopwords) \\\n",
      "                  and ((alnum_only and token.isalnum()) or not alnum_only)\n",
      "                  and ((alpha_only and token.isalpha()) or not alpha_only)]\n",
      "    return tokens\n",
      "\n",
      "def tokenize_sentence_text(sentences, alnum_only = False, alpha_only = False, remove_stopwords=False, use_pattern = 1, show_pbar = False):\n",
      "    sent_tokens = []\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Tokenizing senteces\", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(sentences)).start()\n",
      "    i = 0\n",
      "    for sent in sentences:\n",
      "        sent_token = tokenize_text(sent, alnum_only=alnum_only, alpha_only = alpha_only, remove_stopwords=remove_stopwords, use_pattern=use_pattern)\n",
      "        sent_tokens.append(sent_token)\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return sent_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 183
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_word_count(tokens):\n",
      "    return len(tokens)\n",
      "\n",
      "def get_unique_word_count(tokens):\n",
      "    return len(set(tokens))\n",
      "\n",
      "def get_average_word_length(tokens):\n",
      "    return sum([len(token) for token in tokens])/float(len(tokens))\n",
      "\n",
      "def get_longest_word(tokens, ignore_join_words = True):\n",
      "    if ignore_join_words:\n",
      "        return \", \".join([max([token for token in tokens if token.isalpha()], key=lambda token:len(token))])\n",
      "    else:\n",
      "        return \", \".join([max([token for token in tokens], key=lambda token:len(token))])\n",
      "def print_word_stats(tokens):\n",
      "    print '%-32s' % 'Info type', '%-16s' % 'Value'\n",
      "    print '%-32s' % 'number of words', '%-16d' % get_word_count(tokens)\n",
      "    print '%-32s' % 'number of unique words', '%-16d' % get_unique_word_count(tokens)\n",
      "    print '%-32s' % 'average word length', '%-16.2f' % get_average_word_length(tokens)\n",
      "    #exclude joint words for longest word\n",
      "    print '%-32s' % 'longest single word', '%-16s' % get_longest_word(tokens, ignore_join_words = True)\n",
      "    print '%-32s' % 'longest word', '%-16s' % get_longest_word(tokens, ignore_join_words = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 184
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part of Speech Taggers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_backoff_tagger (train_sents, default='NN'):\n",
      "    t0 = nltk.DefaultTagger(default)\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    return t2\n",
      "def get_brown_tagger(category = None):\n",
      "    if category:\n",
      "        return build_backoff_tagger(brown.tagged_sents(categories=category))\n",
      "    else:\n",
      "        return build_backoff_tagger(brown.tagged_sents())\n",
      "def get_default_treebank_tagger():\n",
      "    return nltk.data.load('taggers\\maxent_treebank_pos_tagger\\english.pickle')\n",
      "\n",
      "def tag_pos_sentences(tokenized_sentences, tagger=get_default_treebank_tagger(), show_pbar = False):\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(tokenized_sentences)).start()\n",
      "    i = 0\n",
      "    tagged_sentences = []\n",
      "    for sent in tokenized_sentences:\n",
      "        tagged_sentences.append(tagger.tag(sent))\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return tagged_sentences\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 185
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text Chunker"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_chunker(grammer=None, tag_set=None):\n",
      "    if not grammer:\n",
      "        if tag_set is None or type=='treebank':\n",
      "            grammer = r\"\"\"PNS: {<DT|JJ|NN|NNS|NNP|NNPS>+<IN>*<DT|JJ|NN|NNS|NNP|NNPS>*<NNP|NNPS><CD>*}\n",
      "                          {<DT|JJ|NN|NNS|NNP|NNPS>*<NNP|NNPS><CD>*}\n",
      "                            \"\"\"\n",
      "        elif tag_set=='brown':\n",
      "            DJNP = \"<DT|JJ|JJ\\$|JJ+JJ|JJR+CS|JJS|JJT|JJ-TL|JJ-HL|FW-JJ|FW-JJR|VBG-TL|VBN-TL\"\\\n",
      "                +\"|NN|NN\\$|NN\\$-TL|NN-HL|NN-TL|NNS|NNS-HL|NNS-TL|NNS-TL-HL|FW-NN|FW-NNS|FW-NN-TL\"\\\n",
      "                +\"|NP|NP\\$|NP-HL|NP-TL|NPS|FW-NP|FW-NPS>\"\n",
      "            #proper nouns\n",
      "            NPS = \"<NP|NP\\$|NP-HL|NP-TL|NPS|FW-NP|FW-NPS>\" \n",
      "\n",
      "            grammer = r\"PNS: {\"+DJNP+\"+<IN|IN-TL>*\"+DJNP+\"*\"+NPS+\"<CD|MD>*}\"+\"\\n{\"+DJNP+\"*\"+NPS+\"<CD|CD-TL|MD>*}\" \n",
      "        elif tag_set=='brown_simple':\n",
      "            grammer = r\"\"\"PNS: {<DET|ADJ|N>*<NP><NUM>*}\n",
      "                            {<DET|ADJ|N>+<IN>*<DET|ADJ|N><NP><NUM>*}\"\"\"\n",
      "        \n",
      "    return nltk.RegexpParser(grammer)\n",
      "\n",
      "def get_chunks(tagged_sents, chunker=get_chunker(), target = 'PNS', \\\n",
      "                     print_output=True, limit=0, randomize= False, show_pbar = False):\n",
      "    chunks = []\n",
      "    limit = limit if limit>0 else len(tagged_sents)\n",
      "    i=0\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=limit).start()\n",
      "    import random\n",
      "    for index in range(limit):\n",
      "        sent_index = random.randint(0, len(tagged_sents)-1) if randomize else index\n",
      "        sent = tagged_sents[sent_index]\n",
      "        result = chunker.parse(sent) \n",
      "        phrase_leaves = [subtree.leaves() for subtree in  result.subtrees() if subtree.node==target]\n",
      "        words = [\" \".join([word.encode('utf-8') for (word,tag) in leaf]) for leaf in phrase_leaves ]\n",
      "        if len(words)>0:\n",
      "            chunks.append(words)\n",
      "        if print_output:\n",
      "            print \"\\t\".join([word for (word, tag) in sent])\n",
      "            print \"\\t\".join([tag for (word, tag) in sent])\n",
      "            print \"\\n\".join(['%d\\t%s'%(j+1, words[j]) for j in range(len(words))])\n",
      "            print '\\n'\n",
      "#         print result\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return [phrase for phrases in chunks for phrase in phrases]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 186
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Printing Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_FreqDist(fd, limit =0):\n",
      "    if limit == 0:\n",
      "        limit = len(fd.items())\n",
      "    print \"\\n\".join([\"%d\\t%s\"%( value, word) for (word, value) in fd.items()[:limit]])\n",
      "#prints multiple frequency distribution next to each other to compare results.\n",
      "def print_FreqDists(fds, titles=None, limit = 50, csv=False):\n",
      "    lines = ''\n",
      "    html_str =''\n",
      "    if limit ==0:\n",
      "        limit = max([len(fd) for fd in fds])\n",
      "    titles = titles if titles else [i for i in range(len(fds))]\n",
      "    lines+=\",\".join(['phrase %s,frequency %s'%(t,t) for t in titles])+'\\n'\n",
      "    for i in range(limit):\n",
      "        line = '%d'%(i+1)\n",
      "        for fd in fds:\n",
      "            key = ''\n",
      "            val = 0\n",
      "            if i < len(fd.items()):\n",
      "                key =fd.items()[i][0]\n",
      "                val = fd.items()[i][1]\n",
      "            if csv:\n",
      "                line='%s,%s,%d'%(line,key,val)\n",
      "            else:\n",
      "                line += '%d\\t%s\\t'%(val, key)\n",
      "       \n",
      "#         print line\n",
      "        lines+='%s\\n'%line\n",
      "    return lines\n",
      "\n",
      "def print_csv_table(csv_lines):\n",
      "    import pandas, io\n",
      "    if isinstance(csv_lines, list):\n",
      "        csv_lines = str(\"\\n\".join(csv_lines))\n",
      "    plines= pandas.read_csv(io.BytesIO(csv_lines))\n",
      "    plines\n",
      "\n",
      "def print_pos_tagged_sentences(tagged_sentences):\n",
      "    formatted_sents = []\n",
      "    import pandas, io\n",
      "    for sent in tagged_sentences:\n",
      "        words = [word for (word, tag) in sent]\n",
      "        tags = [tag for (word, tag) in sent]\n",
      "        csv_line = \"%s\\n%s\"%(\",\".join(words), \",\".join(tags))\n",
      "        plines=pandas.read_csv(io.BytesIO(str(csv_line).decode('utf-8')))\n",
      "        plines\n",
      "        formatted_sents.append(\",\".join(words))\n",
      "        formatted_sents.append(\",\".join(tags))\n",
      "#         formatted_sents.append('\\n')\n",
      "#     print \"\\n\".join(formatted_sents)\n",
      "#     print_csv_table(formatted_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# files = load_xml_files('ENERGY')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sents = flatten_document_structure_long(files)\n",
      "# sents[:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    }
   ],
   "metadata": {}
  }
 ]
}