{
 "metadata": {
  "gist_id": "6726db02bf6bdfbc209a",
  "name": "",
  "signature": "sha256:8a9e312627afab4f40484dc963b7ddaaabf4fb432e8e9372255133d398007acc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Multi United Nations Corpus Sever Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a list of all the service functions used to access and process the Multi UN corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from __future__ import absolute_import\n",
      "# from __future__ import division, unicode_literals\n",
      "import nltk\n",
      "import re\n",
      "import os\n",
      "import chardet\n",
      "import string\n",
      "import inspect\n",
      "from unidecode import unidecode\n",
      "from progressbar import AnimatedMarker, Bar, BouncingBar,\\\n",
      "                            Counter, ETA, Percentage, ProgressBar, SimpleProgress, FileTransferSpeed\n",
      "from nltk.corpus import brown\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Progress Bar Settings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_pbars = True\n",
      "def disable_pbars():\n",
      "    print 'Disabling progress bars'\n",
      "    global show_pbars\n",
      "    show_pbars = False\n",
      "def is_show_pbars():\n",
      "    global show_pbars\n",
      "#     print 'show pbars is ' , show_pbars\n",
      "    return show_pbars"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Data Path"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FILENAME = inspect.getframeinfo(inspect.currentframe()).filename\n",
      "F_PATH = os.path.dirname(os.path.abspath(FILENAME))\n",
      "ROOT_CORPUS_DIR = ''\n",
      "RELATIVE_PATH_TO_XML = 'data/multiUN.en/un/xml/en'\n",
      "RELATIVE_PATH_TO_TXT = 'data/multiUN.en/un/txt/en'\n",
      "PATH_TO_FILES = os.path.abspath(os.path.join(F_PATH, '..', RELATIVE_PATH_TO_TXT))\n",
      "# PATH_TO_XML_FILES=os.path.join(\"..\",\"data\",\"multiUN.en\",\"un\",\"xml\",\"en\")\n",
      "PATH_TO_XML_FILES =  os.path.abspath(os.path.join(F_PATH, '..', RELATIVE_PATH_TO_XML))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fix Unicode and Incomplete Sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fix_unicode(s):\n",
      "    text = ''\n",
      "    try:\n",
      "        text = str(s)\n",
      "        return text\n",
      "    except:\n",
      "        try:\n",
      "#             print 'encoding:', chardet.detect(s)['encoding']\n",
      "            text = s.encode(chardet.detect(s)['encoding'])\n",
      "            return text\n",
      "        except Exception, e:\n",
      "#             print e\n",
      "            try:\n",
      "                text = unidecode(s)\n",
      "                return text\n",
      "#                 f.write(sent)\n",
      "            except Exception, e2:\n",
      "#                 print 'unidecode:', e2\n",
      "                print 'error: ', e2\n",
      "                print s\n",
      "                return text\n",
      "                pass\n",
      "INCOMPLETE_SUFFIXES = ['Mr.', 'Ms.', 'Ch.']\n",
      "def fix_incomplete_sentences(para):\n",
      "    \n",
      "    sents = []\n",
      "    for i in range(len(para)):\n",
      "        sent = para[i]\n",
      "        out_sent=para[i]\n",
      "        while any(sent.endswith(suffix) for suffix in INCOMPLETE_SUFFIXES) and i<(len(para)-1):\n",
      "            i+=1\n",
      "            out_sent+=' %s'%(para[i].strip())\n",
      "            sent=para[i]\n",
      "        sents.append(out_sent)\n",
      "    return sents\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_files(year = None, raw=True):\n",
      "    years = []\n",
      "    if year is None:\n",
      "        years = [ year for year in os.listdir(PATH_TO_FILES) if not '.txt' in year or '_OLD_' in year]\n",
      "    else:\n",
      "        years = [str(year)]\n",
      "    data = {}\n",
      "    pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(years)).start()\n",
      "    for i in range(len(years)):\n",
      "        y = years[i]\n",
      "        data.update(load_files_by_year(y, raw))\n",
      "        pbar.update(i+1)\n",
      "    pbar.finish()\n",
      "    return data\n",
      "\n",
      "\n",
      "def load_files_by_year(year, raw=True):\n",
      "    texts = []\n",
      "    file_type = 'raw' if raw else 'txt'\n",
      "    full_path = os.path.join(PATH_TO_FILES, year, file_type)        \n",
      "    files = os.listdir(full_path)\n",
      "    pbar2 = ProgressBar(widgets=[year, SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(files)).start()\n",
      "    for i in range(len(files)):\n",
      "        filename = os.path.join(full_path, files[i])\n",
      "        with open(filename, 'r') as f:\n",
      "            text = f.read()\n",
      "            text = text.decode('utf-8') #the regular text was throwing an exception complaining about ascii\n",
      "            texts.append(text) #Keeping each file in a seperate array element\n",
      "            pbar2.update(i+1)\n",
      "    pbar2.finish()\n",
      "    return texts    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Load XML Files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree\n",
      "#data ={}\n",
      "def load_xml_files(year = None, path = PATH_TO_XML_FILES, show_pbar=None, content=True):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    data = {}\n",
      "    years = []\n",
      "    if year is None:\n",
      "        years = [ year for year in os.listdir(path)]\n",
      "    else:\n",
      "        years = [str(year)]\n",
      "    \n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(years)).start()\n",
      "    for i in range(len(years)):\n",
      "        y = years[i]\n",
      "        data.update(load_xml_files_by_year(y, path, show_pbar, content))\n",
      "        if show_pbar:\n",
      "            pbar.update(i+1)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return data\n",
      "\n",
      "def load_xml_files_by_year(year, path = PATH_TO_XML_FILES, show_pbar = None, content=True):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    documents = {}\n",
      "    full_path = os.path.join(path, year)    \n",
      "#     print full_path\n",
      "    files = [fname for fname in os.listdir(full_path) if fname.endswith('.xml')]\n",
      "    if show_pbar and len(files)>0:\n",
      "        pbar2 = ProgressBar(widgets=[year, SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(files)).start()\n",
      "    for i in range(len(files)):\n",
      "        f = files[i]\n",
      "        filename = os.path.join(full_path, f)\n",
      "        documents[f] = load_xml_file(filename=filename, content=content)\n",
      "        if show_pbar:\n",
      "            pbar2.update(i+1)\n",
      "    if show_pbar and len(files)>0:\n",
      "        pbar2.finish()\n",
      "    return documents\n",
      "\n",
      "def get_year_from_filename(filename):\n",
      "    sep = '\\\\' if '\\\\' in filename else '/'\n",
      "    return str(filename.split(sep)[-2:-1][0])\n",
      "\n",
      "\n",
      "def load_xml_file(filename = None, content=True, year = None):\n",
      "    f = {}\n",
      "    tree = etree.parse(filename)\n",
      "    root = tree.getroot()\n",
      "    f=dict(root.attrib)\n",
      "    f['year'] = year if year else get_year_from_filename(filename)\n",
      "    if content:\n",
      "        xparas =  root.getchildren()[0].getchildren()[0].getchildren()\n",
      "        content = []\n",
      "        for xpara in xparas:\n",
      "          content.append([fix_unicode(sent.text.strip()) for sent in xpara if len(sent.text.strip())>0])\n",
      "        f['content'] =  content\n",
      "    return f\n",
      "\n",
      "# def flatten_document_structure_paragraphs(doc_dict):\n",
      "#     print 'flattening paragraphs'\n",
      "#     flat = [fix_incomplete_sentences(para) for doc in doc_dict for para in doc_dict[doc]['content']]\n",
      "#     return flat\n",
      "\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Load Document Map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json, zipfile\n",
      "RELATIVE_PATH_TO_MAP = 'util/MUN_MAP.zip'\n",
      "PATH_TO_MAP = os.path.abspath(os.path.join(F_PATH, '..', RELATIVE_PATH_TO_MAP))\n",
      "map_zip = zipfile.ZipFile(PATH_TO_MAP)\n",
      "MUN_MAP = None\n",
      "MAP_FILE = 'map.json'\n",
      "\n",
      "DOC_ID_MAP = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def change_map_file(map_file=MAP_FILE):\n",
      "    global MAP_FILE\n",
      "    MAP_FILE = map_file\n",
      "    print 'Chaged map file to ', map_file\n",
      "    \n",
      "    \n",
      "def load_doc_map(path_prefix=None):\n",
      "    global MUN_MAP\n",
      "    global DOC_ID_MAP\n",
      "    if MUN_MAP is None:\n",
      "        MUN_MAP = json.loads(map_zip.read(MAP_FILE,'r'))\n",
      "        DOC_ID_MAP = dict(sorted([(MUN_MAP[doc]['attributes']['id'].strip(), doc) for doc in MUN_MAP], reverse=True))\n",
      "        if path_prefix is not None:\n",
      "            print 'Changing paths to ', path_prefix\n",
      "            for k,doc in MUN_MAP.iteritems():\n",
      "                MUN_MAP[k]['attributes']['path'] = '%s/%s'%(path_prefix, MUN_MAP[k]['attributes']['path'])\n",
      "\n",
      "        \n",
      "#     return MUN_MAP\n",
      "\n",
      "\n",
      "def validate_search_term(doc, term=None, doc_name=None, doc_id = None, doc_n=None, filename = None, title=None):\n",
      "    load_doc_map()\n",
      "    if doc is not None and doc==doc_name:\n",
      "        return True\n",
      "    if doc_id is not None:\n",
      "        return doc_id in DOC_ID_MAP\n",
      "#     if term is None and doc_name is not None:\n",
      "#         term = doc_name\n",
      "    \n",
      "    return (\n",
      "                term is not None and\n",
      "                (term in MUN_MAP[doc]['attributes']['n']\n",
      "                or term in MUN_MAP[doc]['attributes']['id']\n",
      "                or doc.endswith(term)\n",
      "                or ( 'scrape' in MUN_MAP[doc]  \n",
      "                 and (( 'Title' in MUN_MAP[doc]['scrape'] and term.lower() \n",
      "                       in (MUN_MAP[doc]['scrape']['Title']).lower())\n",
      "                      or ('Subjects' in MUN_MAP[doc]['scrape'] and term.lower() \n",
      "                          in \" \".join(MUN_MAP[doc]['scrape']['Subjects']).lower()))\n",
      "                    )\n",
      "                )\n",
      "                or (title is not None \n",
      "                    and 'scrape' in MUN_MAP[doc]   \n",
      "                    and 'Title' in MUN_MAP[doc]['scrape'] \n",
      "                    and title.lower() in MUN_MAP[doc]['scrape']['Title'].lower())\n",
      "                or doc_n==MUN_MAP[doc]['attributes']['n']\n",
      "                or doc_id == MUN_MAP[doc]['attributes']['id']\n",
      "            )\n",
      "\n",
      "\n",
      "def load_contents(docs):\n",
      "    if docs is not None:\n",
      "        for doc in docs:\n",
      "            path = docs[doc]['attributes']['path']\n",
      "            doc_data = load_xml_file(filename = path, content=True)\n",
      "            if doc_data: \n",
      "                docs[doc]['content'] = doc_data['content']\n",
      "    return docs\n",
      "\n",
      "\n",
      "def get_documents(term = None, doc_name = None, doc_id=None, doc_n=None, filename = None, title=None, limit = None, \n",
      "                  include_content = False):\n",
      "    load_doc_map()\n",
      "    if doc_name is not None and doc_name in MUN_MAP:\n",
      "            result =  {doc_name:MUN_MAP[doc_name]}\n",
      "    else:\n",
      "        result = {}\n",
      "        counter = 0\n",
      "        term = term if term is not None else doc_name\n",
      "        for doc in MUN_MAP:\n",
      "            if validate_search_term(doc, term, doc_name, doc_id, doc_n, filename, title):\n",
      "                result[doc] = MUN_MAP[doc]\n",
      "                if 'links' not in result[doc]:\n",
      "                    print result[doc]\n",
      "                    result[doc]['links']=[]\n",
      "                counter+=1\n",
      "            if limit is not None and counter>= limit:\n",
      "                break\n",
      "#     order =  sorted(result, key=lambda d: len(result[d]['links']),reverse=True )\n",
      "#     print order\n",
      "#     result = dict([(d, result[d]) for d in order])\n",
      "    if include_content:\n",
      "        result = load_contents(result)\n",
      "    return result\n",
      "\n",
      "def get_document(term = None, doc_name = None, doc_id=None, doc_n=None, filename = None, title=None, include_content = False):\n",
      "    load_doc_map()\n",
      "    if doc_name and doc_name in MUN_MAP:\n",
      "            result = {doc_name:MUN_MAP[doc_name]}\n",
      "    else:\n",
      "#         term = term if term is not None else doc_name\n",
      "        try:\n",
      "            result =  next({doc:MUN_MAP[doc]} for doc in MUN_MAP if validate_search_term(doc, term, doc_name, doc_id, doc_n, filename, title))\n",
      "        except Exception,e:\n",
      "            return None\n",
      "    if include_content:\n",
      "        result = load_contents(result)\n",
      "    return result\n",
      "\n",
      "\n",
      "def get_subjects(docs=None):\n",
      "    load_doc_map()\n",
      "    subjects = []\n",
      "    if docs is not None:\n",
      "        subjects_list = [MUN_MAP[doc]['scrape']['Subjects']\n",
      "            for doc in docs \n",
      "            if 'scrape' in MUN_MAP[doc]\n",
      "            and 'Subjects' in MUN_MAP[doc]['scrape']]\n",
      "    else:\n",
      "        subjects_list = [MUN_MAP[doc]['scrape']['Subjects']\n",
      "            for doc in MUN_MAP \n",
      "            if 'scrape' in MUN_MAP[doc]\n",
      "            and 'Subjects' in MUN_MAP[doc]['scrape']]\n",
      "    subjects = [ subject for subjects in subjects_list for subject in subjects]\n",
      "    return subjects"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Extract Paragraphs and Sentences"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions to extract sentence or paragraph-sentence lists from document dictionary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_paragraphs(doc_dict, merge_paragraphs=False):\n",
      "    if 'content' in doc_dict:\n",
      "        doc_dict = {'temp':doc_dict}\n",
      "    flat = [fix_incomplete_sentences(para) for doc in doc_dict for para in doc_dict[doc]['content']]\n",
      "    if merge_paragraphs:\n",
      "        flat = [\" \".join(para) for para in flat]\n",
      "    return flat\n",
      "\n",
      "def extract_sentences(doc_dict):\n",
      "    if 'content' in doc_dict:\n",
      "        doc_dict = {'temp':doc_dict}\n",
      "    text = [fix_incomplete_sentences(para) for doc in doc_dict for para in doc_dict[doc]['content']]\n",
      "    text = [sent for para in text for sent in para]\n",
      "        \n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sentence Tokenizers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def parse_sentences_from_text(text, use_nltk_tokenizer = False ):\n",
      "    sents = []\n",
      "    if use_nltk_tokenizer:\n",
      "        sents = sent_tokenizer.sentences_from_text(text)\n",
      "    else:\n",
      "        #split the document by \\n and remove any empty line and extra whitespace\n",
      "        sents = [sent.strip() for sent in text.split('\\n') if len(sent.strip())>0] \n",
      "    return sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sentence Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentence_count(sentences):\n",
      "    return len(sentences)\n",
      "\n",
      "def get_average_words_per_sentence(sentences):\n",
      "    \n",
      "    return sum([len(sent.split(' ')) for sent in sentences])/float(len(sentences))\n",
      "\n",
      "def get_average_characters_per_sentence(sentences):\n",
      "    return sum([len(sent) for sent in sentences])/float(len(sentences))\n",
      "\n",
      "def get_longest_sentence_by_words(sentences):\n",
      "    return max(sentences, key=lambda w:len(w)).split(\" \")\n",
      "\n",
      "def get_longest_sentence_by_characters(sentences):\n",
      "    return max(sentences, key=lambda w:len(w))\n",
      "\n",
      "def print_sentence_statistics(sentences):\n",
      "    print '%-32s' % 'Info type', '%-16s' % 'Value',  '\\n----------------------------------------'\n",
      "    print '%-32s' % 'number of sentences', '%-16d' % len(sentences)\n",
      "    print '%-32s' % 'average sentence length(words)', '%-16.2f' % get_average_words_per_sentence(sentences)\n",
      "    print '%-32s' % 'average sentence length(letters)', '%-16.2f' % get_average_characters_per_sentence(sentences)\n",
      "    print '%-32s' % 'longest sentence (words)', '%-16s' % len(get_longest_sentence_by_words(sentences))\n",
      "    print '%-32s' % 'longest sentence (letters)', '%-16s' % len(get_longest_sentence_by_characters(sentences))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Tokenizer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create word tokens from sentences\n",
      "* pattern1: no punctuation\n",
      "* pattern2: include punctuations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "english_stopwords = stopwords.words('english')\n",
      "\n",
      "    \n",
      "\n",
      "def tokenize_text(text, alnum_only = False,  alpha_only = False,remove_stopwords=False, use_pattern = 2):\n",
      "    pattern1 = [\"\\w+[\\-|']\\w+\", #words joined with '-' or words with ' in them \\\n",
      "               \"(\\w+/)+\\w+\", #document references and words that are seperated with '\\' \\\n",
      "               \"\\([\\w+\\s*]\\)\", #words with parenthesis in them. this will exclude parenthisis \\\n",
      "               \"\\w+\"]\n",
      "\n",
      "    pattern2 =[\"(?x)([A-Z]\\.)+\",\n",
      "               \"\\w+([-']\\w+)*\",\n",
      "               \"\\$?\\d+(\\.\\d+)?%?\",\n",
      "               \"\\.\\.\\.\",\n",
      "               \"[.,?;]+\"]\n",
      "    pattern  = pattern1 if use_pattern ==1 else pattern2\n",
      "    pattern = \"|\".join(pattern)\n",
      "    text = \" \".join(text) if isinstance(text, list)== True else text\n",
      "    tokens = nltk.regexp_tokenize(text,pattern)\n",
      "    tokens =  [token for token in tokens \\\n",
      "                  if ((remove_stopwords and token.lower() not in english_stopwords) or not remove_stopwords) \\\n",
      "                  and ((alnum_only and token.isalnum()) or not alnum_only)\n",
      "                  and ((alpha_only and token.isalpha()) or not alpha_only)]\n",
      "    return tokens\n",
      "\n",
      "\n",
      "# tokenize words but keep sentences seperate\n",
      "def tokenize_sentence_text(sentences, alnum_only = False, alpha_only = False, remove_stopwords=False, \n",
      "                           use_pattern = 1, show_pbar=None):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    \n",
      "    sent_tokens = []\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Tokenizing sentences\", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(sentences)).start()\n",
      "    i = 0\n",
      "    for sent in sentences:\n",
      "        sent_token = tokenize_text(sent, alnum_only=alnum_only, alpha_only = alpha_only, remove_stopwords=remove_stopwords, use_pattern=use_pattern)\n",
      "        sent_tokens.append(sent_token)\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return sent_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Word Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_word_count(tokens):\n",
      "    return len(tokens)\n",
      "\n",
      "def get_unique_word_count(tokens):\n",
      "    return len(set(tokens))\n",
      "\n",
      "def get_average_word_length(tokens):\n",
      "    return sum([len(token) for token in tokens])/float(len(tokens))\n",
      "\n",
      "def get_longest_word(tokens, ignore_join_words = True):\n",
      "    if ignore_join_words:\n",
      "        return \", \".join([max([token for token in tokens if token.isalpha()], key=lambda token:len(token))])\n",
      "    else:\n",
      "        return \", \".join([max([token for token in tokens], key=lambda token:len(token))])\n",
      "def print_word_stats(tokens):\n",
      "    print '%-32s' % 'Info type', '%-16s' % 'Value'\n",
      "    print '%-32s' % 'number of words', '%-16d' % get_word_count(tokens)\n",
      "    print '%-32s' % 'number of unique words', '%-16d' % get_unique_word_count(tokens)\n",
      "    print '%-32s' % 'average word length', '%-16.2f' % get_average_word_length(tokens)\n",
      "    #exclude joint words for longest word\n",
      "    print '%-32s' % 'longest single word', '%-16s' % get_longest_word(tokens, ignore_join_words = True)\n",
      "    print '%-32s' % 'longest word', '%-16s' % get_longest_word(tokens, ignore_join_words = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Part of Speech Taggers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "This is a manually tagged tagger of cities and countries from the gazetteers corpus. I also appended a special tag or United Nations because the tagger kept detecting United as a verb. \n",
      "\n",
      "I am using custom tags (NPLOC and NPORG) because I don't want them to be mixed with other NPs while having the chunker detect them as a sub class of NP. United Nations was especially important because it occurs alot and is United is flagged as a verb which throws off the chunker, especially the verb object chunker.\n",
      "\n",
      "I experimented with The regex tagger only support 100 groups max and the it won't deal with tokenized sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#location/organization tagger\n",
      "def get_location_tagger_tags():\n",
      "    from nltk.corpus import gazetteers as gz\n",
      "    pos_tags_locations =[[('United', 'NPORG'), ('Nations', 'NPORG')],\n",
      "                         [('Working', 'NPORG'), ('Party', 'NPORG')]]\n",
      "    pos_tags_locations +=[[(word, 'NPLOC') for word in words.split(' ') if word not in english_stopwords] for words in gz.words()]\n",
      "    '''\n",
      "#   pos_tags_locations += [[(words, 'NPLOC') ]for words in gz.words() ]\n",
      "#   pos_tags_locations = [(words, 'NPLOC') for words in [gzwords for gzwords in gz.words() if len(gzwords.split(' '))>2]][:90]\n",
      "    pos_tags_locations+=[(r'\\w* Republic \\w*', 'NPLOC'),(r'\\w* Kingdom \\w*', 'NPLOC'),('United Nations', 'NPORG'),('Working Party', 'NPORG')]\n",
      "    '''\n",
      "    \n",
      "    return pos_tags_locations\n",
      "\n",
      "\n",
      "#the tagger here can be a 3 stage tagger or 5 state if the location tagger is included\n",
      "#note that the location tagger is used first to ensure the location/orgainzations are detected properly\n",
      "def build_backoff_tagger (train_sents, default='NN', include_location_tagger=False):\n",
      "    t0 = nltk.DefaultTagger(default)\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    t4=t2\n",
      "    if include_location_tagger:\n",
      "        t3 = nltk.RegexpTagger([(r'UN[A-Z]*', 'NPORG')], backoff=t2)\n",
      "        t4 = nltk.UnigramTagger(get_location_tagger_tags(), backoff=t3) #tag using the custom tagger first if requested\n",
      "    return t4\n",
      "\n",
      "\n",
      "def build_location_tagger():\n",
      "    t0 = nltk.DefaultTagger('U')\n",
      "    return nltk.BigramTagger(get_location_tagger_tags(), backoff=t0)\n",
      "\n",
      "\n",
      "def get_brown_tagger(category = None, include_location_tagger=False):\n",
      "    if category:\n",
      "        return build_backoff_tagger(brown.tagged_sents(categories=category), include_location_tagger=include_location_tagger)\n",
      "    else:\n",
      "        return build_backoff_tagger(brown.tagged_sents(), include_location_tagger=include_location_tagger)\n",
      "\n",
      "    \n",
      "def get_default_treebank_tagger():\n",
      "    return nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n",
      "\n",
      "\n",
      "#Tag the sentences based on the selected tagger\n",
      "def tag_pos_sentences(tokenized_sentences, tagger=get_brown_tagger(), show_pbar=None):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"tagging sentences\", SimpleProgress(), Percentage(), Bar(), ETA()],\n",
      "                           maxval=len(tokenized_sentences)).start()\n",
      "    i = 0\n",
      "    tagged_sentences = []\n",
      "    for sent in tokenized_sentences:\n",
      "        tagged_sentences.append(tagger.tag(sent))\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return tagged_sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text Chunker"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My pride an joy chunker tries to do alot. I experimented heavily with it in previous assignments to ensure it captures complex objects. I am targeting 2 main classes:\n",
      "* **PNS**: Proper nouns which in this case can be as long as 7 words for some UN organizations\n",
      "* **VNS**: Verb noun subjects (or who did what)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_punctuation(text):\n",
      "    return \"\".join(c for c in text if c not in string.punctuation)\n",
      "\n",
      "\n",
      "#it assumes brown tag set and proper nouns as default parameters. I found brown to be best in detecting proper nouns\n",
      "def get_chunker(grammer=None, tag_set=None, target='PNS'):\n",
      "    if not grammer:\n",
      "        #not used here\n",
      "        if tag_set=='treebank':\n",
      "            grammer = r\"\"\"PNS: {<DT|JJ.*|N.*>+<IN>*<DT|J.*|N.*>*<NNP.*><CD>*}\n",
      "                          {<DT|J.*|N.*>*<NNP.*><CD>*}\n",
      "                            \"\"\"\n",
      "        elif tag_set is None or tag_set=='brown':\n",
      "            if target=='PNS':\n",
      "                #words that precede or folllow proper nouns that are part of it (e.g. UN commission for xxx)\n",
      "                DJNP = \"<DT|J.*|FW-J.*|VBG.*|VBN.*\"\\\n",
      "                    +\"|N.*|FW-N.*\"\\\n",
      "                    +\"|NP.*|FW-NP.*>\"\n",
      "                #proper nouns\n",
      "                NPS = \"<NP.*|FW-NP.*>\" \n",
      "\n",
      "                grammer = r\"PNS: {\"+DJNP+\"+<IN.*>*\"+DJNP+\"*\"+NPS+\"<CD|MD>*}\"+\"\\n{\"+DJNP+\"*\"+NPS+\"<CD|CD-TL|MD|N.*>*}\"\n",
      "            \n",
      "            elif target=='VNS':\n",
      "                #verbs and teh different nouns that come before it or after it\n",
      "                grammer  = r\"\"\"VNS: {<N.*|J.*>*<VB.*>+<CD|TO|IN|CC|DT|PRP>*<DT|J.*|N.*>*<N.*|J.*>}\n",
      "                        {<N.*|J.*>+<CD|TO|IN|CC|DT|PRP>*<DT|J.*|N.*>*<VB.*>+}\n",
      "                         \"\"\"\n",
      "        #not used here\n",
      "        elif tag_set=='brown_simple':\n",
      "            grammer = r\"\"\"PNS: {<DET|ADJ|N>*<NP><NUM>*}\n",
      "                            {<DET|ADJ|N>+<IN>*<DET|ADJ|N><NP><NUM>*}\"\"\"\n",
      "    return nltk.RegexpParser(grammer)\n",
      "\n",
      "\n",
      "# returns chunk trees based on teh chunker and target chosesn\n",
      "# randomize and limit are for testing purposues. They allow the chunker to run on a limited number of random sentences\n",
      "def get_chunks(tagged_sents, chunker=get_chunker(), target = 'PNS', limit=0, randomize= False, show_pbar=None):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    chunks = []\n",
      "    limit = limit if limit>0 else len(tagged_sents)\n",
      "    i=0\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Chunking \", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=limit).start()\n",
      "    import random\n",
      "    for index in range(limit):\n",
      "        sent_index = random.randint(0, len(tagged_sents)-1) if randomize else index\n",
      "        sent = tagged_sents[sent_index]\n",
      "        if len(sent)>0:\n",
      "            result = chunker.parse(sent) \n",
      "            chunks.append(result)\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return chunks\n",
      "\n",
      "\n",
      "#extracts the target chunks from the tree. I seperated this because in some cases I need the trees\n",
      "def extract_target_from_chunks(raw_chunks, target='PNS',print_output=False, print_leaves = False):\n",
      "    chunks=[]\n",
      "    for result in raw_chunks:\n",
      "        phrase_leaves = [subtree.leaves() for subtree in  result.subtrees() if subtree.node==target]\n",
      "        if print_leaves:\n",
      "            print phrase_leaves\n",
      "        words = [\" \".join([word for (word,tag) in leaf]) for leaf in phrase_leaves ]\n",
      "        if len(words)>0:\n",
      "            chunks.append(words)\n",
      "        if print_output:\n",
      "            print \"\\t\".join([word for (word, tag) in sent])\n",
      "            print \"\\t\".join([tag for (word, tag) in sent])\n",
      "            print \"\\n\".join(['%d\\t%s'%(j+1, words[j]) for j in range(len(words))])\n",
      "            print '\\n'\n",
      "        \n",
      "    return [phrase for phrases in chunks for phrase in phrases]\n",
      "\n",
      "\n",
      "#flattens the trees but doesn't remove anything. It only groups the target one tuple. \n",
      "# This helps generate collocations based on complex chunker grammer which was very useful\n",
      "def flatten_chunks(chunks, target='PNS'):\n",
      "    flat_chunks = []\n",
      "    for chunk in chunks:\n",
      "        flat_chunk=[]\n",
      "        for n in chunk:\n",
      "            if isinstance(n, tuple):\n",
      "                flat_chunk.append(n)\n",
      "            elif isinstance(n, nltk.tree.Tree):\n",
      "                flat_node = (\" \".join([word for (word, tag) in n.leaves()]), n.node)\n",
      "                flat_chunk.append(flat_node)\n",
      "        flat_chunks.append(flat_chunk)\n",
      "    return flat_chunks\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Process Chunks to generate chunk Frequency Distrubtions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Named Entities using a multi stage chunker\n",
      "* Verb objects"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "   \n",
      "def process_chunks(sentences=None, sent_tokens=None, tagged_sentences = None,  remove_months = True, tagger = None, return_print = True):\n",
      "    if not tagged_sentences:\n",
      "        sent_tokens = sent_tokens if sent_tokens else \\\n",
      "            tokenize_sentence_text(sentences, alnum_only=False, remove_stopwords=False, use_pattern = 2)\n",
      "        tagger = get_brown_tagger(include_location_tagger=True)\n",
      "    tagged_sentences = tagged_sentences if tagged_sentences else tag_pos_sentences(sent_tokens, tagger=tagger)\n",
      "    #get the proper noun chunks from teh chunker\n",
      "    nchunks = get_chunks(tagged_sentences, chunker=get_chunker(tag_set='brown', target='PNS'), target = 'PNS')\n",
      "    nchunks = extract_target_from_chunks(nchunks, target='PNS')\n",
      "    # there are many month names mentioned in the FrewDist, I am removing them because I don' think they add much value\n",
      "    if remove_months:\n",
      "        import calendar\n",
      "        nchunks = [chunk for chunk in nchunks if len(remove_punctuation(chunk))>1 \n",
      "                   and not any(mn in chunk.split(' ') for mn in calendar.month_name)]\n",
      "    \n",
      "    vchunks = get_chunks(tagged_sentences, chunker=get_chunker(tag_set='brown', target='VNS'), target = 'VNS')\n",
      "    vchunks = extract_target_from_chunks(vchunks, target='VNS')\n",
      "#     vchunks = [chunk for chunk in vchunks if chunk not in nchunks]\n",
      "    nchunks_fd = nltk.FreqDist(nchunks)\n",
      "    vchunks_fd = nltk.FreqDist(vchunks)\n",
      "    if return_print:\n",
      "        return print_FreqDists([nchunks_fd, vchunks_fd], titles=['Proper Nouns', 'Verb Objects'], csv=True)\n",
      "    else:\n",
      "        return nchunks_fd.items(), vchunks_fd.items()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Collocations\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two collocation implementations here:\n",
      "* word based: collocation of individual words\n",
      "* chunk based: collocation using a whole chunker grammer element (e.g. PNS). This will treat the whole element as one words and generate collocation of other words with it. this helps especially when dealing with multi-words country or organization names\n",
      "\n",
      "The output is four finders nbests:\n",
      "(bigram, trigram) x (pmi, chi_sq)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.collocations import *\n",
      "#find pure word frequency collocations\n",
      "#it doesn't matter whether I pass tagged sentences or not since I am focusing on words only\n",
      "def get_collocations(sentences=None, sent_tokens=None, filter_limit = 3, finder_limit = 20):\n",
      "#     from nltk.collocations import *\n",
      "    sent_tokens = sent_tokens if sent_tokens else tokenize_sentence_text(sentences,alpha_only = True,\\\n",
      "                                                                         remove_stopwords=True, use_pattern = 1)\n",
      "    word_tokens = [word for sent in sent_tokens for word in sent]\n",
      "#     print word_tokens[:5]\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    finder = BigramCollocationFinder.from_words(word_tokens)\n",
      "    finder3 = TrigramCollocationFinder.from_words(word_tokens)\n",
      "    finder.apply_freq_filter(filter_limit)\n",
      "    finder3.apply_freq_filter(filter_limit)\n",
      "    #print 4 finders (bigram, trigram) x (pmi, chi_sq)\n",
      "    f1= finder.nbest(bigram_measures.pmi, finder_limit)\n",
      "    f2= finder.nbest(bigram_measures.chi_sq, finder_limit)\n",
      "    f3= finder3.nbest(trigram_measures.pmi, finder_limit)\n",
      "    f4= finder3.nbest(trigram_measures.chi_sq, finder_limit)\n",
      "    return f1, f2,f3\n",
      "\n",
      "#get the colloations based on chunks    \n",
      "def get_chunked_collocations(sentences=None,tagged_sentences=None, tagger=None, chunks=None,\n",
      "                             target='PNS', chunker = None, filter_limit = 3, finder_limit = 20):\n",
      "\n",
      "    #I can pass pre-tagged sentences if needed\n",
      "    if tagger is None:\n",
      "        get_brown_tagger(include_location_tagger=True)\n",
      "    if sentences:\n",
      "        sent_tokens = tokenize_sentence_text(sentences, alnum_only=False, remove_stopwords=False, use_pattern = 2)\n",
      "        tagged_sentences = tag_pos_sentences(sent_tokens, tagger=tagger)\n",
      "    if chunks is None:\n",
      "        chunker = chunker if chunker else get_chunker(tag_set='brown', target = target)\n",
      "        chunks = get_chunks(tagged_sentences,chunker=chunker, target = target)\n",
      "        flat_chunks = flatten_chunks(chunks, target=target)\n",
      "        flat_chunk_tokens = [token for flat_chunk in flat_chunks for token in flat_chunk]\n",
      "        print flat_chunk_tokens\n",
      "    else:\n",
      "        chunks = [c for c,i in chunks]\n",
      "        flat_chunk_tokens = [token for flat_chunk in chunks for token in flat_chunk.split(' ')]\n",
      "        print flat_chunk_tokens\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    finder = BigramCollocationFinder.from_words(flat_chunk_tokens)\n",
      "    finder3 = TrigramCollocationFinder.from_words(flat_chunk_tokens)\n",
      "    \n",
      "    #after some testing, I found that many collocations are numeric so I am filtering out non alpha words\n",
      "    #This filters out any pairs that don't included the target chunk grammer\n",
      "    finder.apply_ngram_filter(lambda (w1, t1), (w2,t2): target not in (t1,t2) or not w1.isalpha() or not w2.isalpha())\n",
      "    finder3.apply_ngram_filter(lambda (w1, t1), (w2,t2), (w3, t3): \\\n",
      "                               target not in (t1,t2,t3) or not w1.isalpha() or not w2.isalpha() or not w3.isalpha())\n",
      "    finder.apply_freq_filter(filter_limit)\n",
      "    finder3.apply_freq_filter(filter_limit)\n",
      "    #print 4 finders (bigram, trigram) x (pmi, chi_sq)\n",
      "    f1= finder.nbest(bigram_measures.pmi, finder_limit)\n",
      "    f2= finder.nbest(bigram_measures.chi_sq, finder_limit)\n",
      "    f3= finder3.nbest(trigram_measures.pmi, finder_limit)\n",
      "    f4= finder3.nbest(trigram_measures.chi_sq, finder_limit)\n",
      "    return f1, f2,f3\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequent Terms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These modules help generate an ngram frequncy distribution. It takes n as an input to plut any number ngrams.\n",
      "\n",
      "It also takes other options about the text (e.g. remove stopwords or lower case everything). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.snowball import SnowballStemmer\n",
      "snowball_stemmer = SnowballStemmer(\"english\")\n",
      "\n",
      "\n",
      "def get_normalized_word(token,stem_words=False, lower_case=False):\n",
      "    if stem_words and len(token)>1:\n",
      "        try:\n",
      "            return snowball_stemmer.stem(token)\n",
      "        except Exception, e:\n",
      "            print token, e\n",
      "            return token\n",
      "    elif lower_case:\n",
      "        return token.lower()\n",
      "    else:\n",
      "        return token\n",
      "    #I wanted to keep CAP words CAPs because the mostly reflect abbreviations but some common words appear as all caps in headers\n",
      "    '''if token.isupper(): \n",
      "        return token\n",
      "    else:\n",
      "        return token.lower()\n",
      "        '''\n",
      "   \n",
      "    \n",
      "def get_normalzed_ngram(ngram, stem_words=False, lower_case=False):\n",
      "    return [get_normalized_word(word, stem_words, lower_case) for word in ngram]\n",
      "\n",
      "#it can take either tokeinzed or text sentences. the output is a frequency distribution of ngrams\n",
      "def get_frequent_ngrams(sentences=None, sent_tokens=None, ngram_length = 1, alnum_only = False, remove_stopwords=False, stem_words = False, lower_case=False):\n",
      "    sent_tokens = sent_tokens if sent_tokens else tokenize_sentence_text(sentences, alnum_only=alnum_only, remove_stopwords=remove_stopwords, use_pattern = 2)\n",
      "    ngrams = [\" \".join(get_normalzed_ngram(ngram, stem_words, lower_case))  \\\n",
      "                       for sent in sent_tokens for ngram in nltk.ngrams(sent, ngram_length) ]\n",
      "    fd_ngrams = nltk.FreqDist(ngrams)\n",
      "    return fd_ngrams\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text Summarization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Manual Summerizations (TO DOCUMENT)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SUMMARY_KEYWORDS_START=['in order to', 'thus', 'to sum up', 'finally', 'in conclusion', 'to conclude', 'to summerize', 'in summary', 'therefore'\n",
      "                'goal', 'the ultimate goal', 'finally']\n",
      "SUMMARY_KEYWORDS_BODY = ['goal', 'summar', 'conclu', 'goal',' aim','objective', 'decide', 'decision', 'focus', 'recommen', 'request']\n",
      "\n",
      "def get_document_summary(doc):\n",
      "    if 'content' not in doc:\n",
      "        doc = doc.itervalues().next()\n",
      "    # got the document title\n",
      "    \n",
      "    paragraphs = extract_paragraphs(doc)\n",
      "    sentences = extract_sentences(doc)\n",
      "    \n",
      "    #get first sentence if each paragraph that is not a heading\n",
      "    sent_first = [p[0] for p in paragraphs if not is_heading(p) and not p[0].startswith('(')]\n",
      "    sent_first_tokenized = tokenize_sentence_text(sent_first, remove_stopwords=True)\n",
      "    \n",
      "    #get title bigrams and trigrams\n",
      "    title = doc['scrape']['Title']\n",
      "    title_bigrams = get_frequent_ngrams(sentences=[title], ngram_length=2, remove_stopwords=True)\n",
      "    title_trigrams = get_frequent_ngrams(sentences=[title], ngram_length=3, remove_stopwords=True)\n",
      "    \n",
      "    # get bigrams and trigrams from document\n",
      "    bigrams = get_frequent_ngrams(sentences=sentences, ngram_length=2, remove_stopwords=True)\n",
      "    trigrams = get_frequent_ngrams(sentences=sentences, ngram_length=3, remove_stopwords=True)\n",
      "    \n",
      "    # get the first paragraph after specific headings\n",
      "    intro = get_after_heading('introduction', paragraphs)\n",
      "    summary = get_after_heading('summary', paragraphs)\n",
      "    conclusion = get_after_heading('conclusion', paragraphs)\n",
      "    \n",
      "    #extract features from the document\n",
      "    features = [(i,check_summary_features(sent_first[i], bigrams, trigrams, title_bigrams,title_trigrams)) \n",
      "                for i in range(len(sent_first))]\n",
      "    \n",
      "    #sort the features by total\n",
      "    sorted_features = sorted(features, key=lambda (i,f): f['total'], reverse=True)\n",
      "    \n",
      "    #get top 10 sentences\n",
      "    top_features =sorted_features[:10]\n",
      "    top_sents = [ (i, r['sent']) for i, r in top_features\n",
      "                     if (len(intro)>0 and r['sent'] != intro[0][1][0])\n",
      "                     or (len(summary)>0 and r['sent'] != summary[0][1][0])\n",
      "                     or (len(summary)>0 and r['sent'] != summary[0][1][0])]\n",
      "    \n",
      "    #flattent the paragraphs\n",
      "    intro = [(i, \" \".join(f)) for i,f in intro]\n",
      "    summary = [(i, \" \".join(f)) for i,f in summary]\n",
      "    conclusion = [(i, \" \".join(f)) for i,f in conclusion]\n",
      "    \n",
      "    \n",
      "    #combine\n",
      "    summary = intro  + top_sents + summary + conclusion\n",
      "    summary = sorted(summary)\n",
      "    return summary\n",
      "    \n",
      "    \n",
      "def check_summary_features(sent, bigrams, trigrams, title_bigrams, title_trigrams):\n",
      "    result={}\n",
      "#     result = {'in_title':0, 'tri':0, 'keyword':0, 'keyword_s':0} \n",
      "    result['in_title_bigrams'] =sum([1 for bi, freq in title_bigrams.items()[:20] if bi.lower() in sent.lower()])\n",
      "    result['in_title_trigrams'] =sum([1 for tri, freq in title_trigrams.items()[:20] if tri.lower() in sent.lower()])*1.5\n",
      "    result['trigrams'] = sum([1 for tri, freq in trigrams.items()[:20] if tri.lower() in sent.lower()])*1.5\n",
      "    result['bigrams'] = sum([1 for bi, freq in bigrams.items()[:20] if tri.lower() in sent.lower()])\n",
      "    result['keyword_s'] = sum([1 for k in SUMMARY_KEYWORDS_START if k in sent.lower()])*3\n",
      "    result['keyword'] = sum([1 for k in SUMMARY_KEYWORDS_BODY if k in sent.lower()])\n",
      "    total = sum([v for v in result.itervalues()])\n",
      "#     print total\n",
      "    result['total'] = total\n",
      "    result['sent'] = sent\n",
      "    return result\n",
      "\n",
      "\n",
      "def get_after_heading(heading,paras):\n",
      "    for i in range(len(paras)):\n",
      "        p = paras[i]\n",
      "        if is_heading(p) and \" \".join(p).lower()==heading.lower() and  i<len(paras):\n",
      "            return [(i+1, paras[i+1])] \n",
      "    return []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text Summarization using Sumy Package"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Sumy requires the following imports'''\n",
      "\n",
      "\n",
      "from sumy.parsers.html import HtmlParser\n",
      "from sumy.parsers.plaintext import PlaintextParser\n",
      "from sumy.nlp.tokenizers import Tokenizer\n",
      "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
      "from sumy.nlp.stemmers import Stemmer\n",
      "from sumy.utils import get_stop_words\n",
      "\n",
      "def sumy_paragraphs(paragraphs, sentence_count=5):\n",
      "    text = \"\\n\".join([\" \".join(p) for p in paragraphs])\n",
      "    LANGUAGE = \"english\"  \n",
      "    parser = PlaintextParser.from_string(text,  Tokenizer(LANGUAGE))\n",
      "    stemmer = Stemmer(LANGUAGE)\n",
      "    summarizer = Summarizer(stemmer)\n",
      "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
      "    summary_of_doc = [str(sent) for sent in summarizer(parser.document, sentence_count)]\n",
      "    return summary_of_doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Named Entity Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "import pycountry as pc\n",
      "\n",
      "\n",
      "# Creating a master list of countries to compare to and filter from the library pycontry. \n",
      "# Defines a dictionary with two arrays, common names and official names.\n",
      "\n",
      "def load_countries():\n",
      "    countries = {\n",
      "        'common':[],\n",
      "        'official': []\n",
      "    }\n",
      "\n",
      "    for c in pc.countries:\n",
      "        countries['common'].append(c.name)\n",
      "        if hasattr(c,'official_name'):\n",
      "            countries['official'].append(c.official_name)\n",
      "    print \"Load_countries() DONE!\"\n",
      "    return countries\n",
      "\n",
      "COUNTRIES = load_countries()\n",
      "# countries\n",
      "\n",
      "# Processes the tagged sentences to get NER entities for analysis. \n",
      "def load_ner_entities(tagged_sentences, show_pbar = None):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Building Dictionary\", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(tagged_sentences)).start()\n",
      "    print \"get_ner_dictionary_for_analysis() Started...\"\n",
      "    entities = []\n",
      "    counter = 0\n",
      "    for sentence in tagged_sentences:\n",
      "        chunks = nltk.ne_chunk(sentence)\n",
      "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'node')])\n",
      "        counter+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(counter)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return entities\n",
      "\n",
      "\n",
      "def get_ner_dictionary_for_analysis(tagged_sentences=None, entities = None, show_pbar = None):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    if entities is None:\n",
      "        entities = load_ner_entities(tagged_sentences, show_pbar)\n",
      "        \n",
      "    # Creating NER main dictionary to analyze and join with MUN library results\n",
      "    ner_dictionary = {\n",
      "        'GPE':[],\n",
      "        'PERSON' :[],\n",
      "        'ORGANIZATION' :[],\n",
      "        'GSP':[]\n",
      "    }\n",
      "    \n",
      "    for e in entities:\n",
      "        if not ner_dictionary.has_key(e.node):\n",
      "            ner_dictionary[e.node]=[]\n",
      "        phrase =[]\n",
      "        for item in e:\n",
      "            phrase.append(item[0])\n",
      "        ner_dictionary[e.node].append(' '.join(phrase))\n",
      "    \n",
      "    return ner_dictionary,entities\n",
      "\n",
      "\n",
      "#Selects the countries from the noun phrases detected that match the official list of countries from pycountry\n",
      "def get_filtered_countries(chunks):\n",
      "    print \"get_filtered_countries() Started...\"\n",
      "    filtered_countries = []\n",
      "    fd = nltk.FreqDist(chunks)\n",
      "    for f in fd.items():\n",
      "        for country in COUNTRIES['common']:\n",
      "            c = fix_unicode(f[0])\n",
      "            if country.find(c) != -1:\n",
      "                filtered_countries.append(f)\n",
      "\n",
      "    filtered_countries = nltk.FreqDist(filtered_countries).items()\n",
      "    filtered_countries = [w[0] for w in filtered_countries]\n",
      "    print \"get_filtered_countries() DONE!\"\n",
      "    return filtered_countries\n",
      "\n",
      "\n",
      "#Creating a Frequency Distribution with the NER entities\n",
      "def get_ner_entities_list(ner_entities):\n",
      "    print \"get_ner_entities_list() Started...\"\n",
      "    entities = []\n",
      "    for node in ner_entities:\n",
      "        phrase = []\n",
      "        for element in node:\n",
      "            phrase.append(element[0])\n",
      "        entities.append((node.node,' '.join(phrase)))\n",
      "    print \"get_ner_entities_list() DONE!\"\n",
      "    return entities\n",
      "\n",
      "\n",
      "# Returns the list of countries/continents joined from the NER GEP list and the MUN Library chunker.\n",
      "\n",
      "def get_ner_countries(nchunks, ner_chunks):\n",
      "    print \"get_ner_countries() Started...\"\n",
      "    all_countries = get_filtered_countries(nchunks) + get_filtered_countries(ner_chunks)\n",
      "    fd = nltk.FreqDist(all_countries)\n",
      "    all_countries = []\n",
      "    for c in fd.keys():\n",
      "        all_countries.append(c[0])\n",
      "#     fd = nltk.FreqDist(all_countries).keys()\n",
      "#     fd.sort()\n",
      "    print \"get_ner_countries() DONE!\"\n",
      "    return nltk.FreqDist(all_countries).items()\n",
      "\n",
      "\n",
      "def get_ner_organizations(nchunks,ner_chunks):\n",
      "    print \"get_ner_organizations() Started...\"\n",
      "    unorgs = ['Commission','General Assembly','Secretariat','Committee', 'United Nations', 'Assembly','Convention ']\n",
      "    norgs = [(w[0][1],w[1]) for w in ner_chunks if w[0][0] == 'ORGANIZATION']\n",
      "    all_orgs = nchunks + norgs\n",
      "    all_orgs = [w[0] for w in all_orgs]\n",
      "    all_orgs = nltk.FreqDist(all_orgs).keys()\n",
      "    results = []\n",
      "    for w in all_orgs:\n",
      "        for org in unorgs:\n",
      "            if type(w) == tuple:\n",
      "                print w\n",
      "            if w.find(org) !=-1 and analyze_for_monograms(w) and w != org:\n",
      "                results.append(w)\n",
      "    print \"get_ner_organizations() DONE!\"\n",
      "    return nltk.FreqDist(results).items()\n",
      "\n",
      "\n",
      "# Checks a text to make sure it isn't just a \n",
      "def analyze_for_monograms(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "    if len(tokens[0])>1 and len(tokens[-1])>1:\n",
      "        return True\n",
      "    else: \n",
      "        return False\n",
      "\n",
      "\n",
      "def ner_document_analysis(sentences, tagged_sentences, nchunks=None, summary=False):\n",
      "\n",
      "    if nchunks == None:\n",
      "        nchunks = get_chunks(tagged_sentences, chunker=get_chunker(tag_set='brown', target='PNS'), target = 'PNS')\n",
      "        nchunks = extract_target_from_chunks(nchunks, target='PNS')\n",
      "        nchunks = nltk.FreqDist(nchunks).items()\n",
      "    \n",
      "    \n",
      "    ner_dictionary,ner_entities = get_ner_dictionary_for_analysis(tagged_sentences)\n",
      "    \n",
      "    fd_gpe = nltk.FreqDist(ner_dictionary['GPE'])\n",
      "    ner_fd_entities = nltk.FreqDist(get_ner_entities_list(ner_entities))\n",
      "    \n",
      "    \n",
      "    def filter_fd(fd, l):\n",
      "        result = [(f,i) for (f,i) in fd if not any([li.lower() in f.lower() or f.lower() in li.lower() for li, li2 in l])]\n",
      "        return result\n",
      "    \n",
      "    def count_fd(fd):\n",
      "        return nltk.FreqDist([f for f,i in fd for sent in sentences if f in sent]).items()\n",
      "    \n",
      "    allcountries = get_ner_countries(nchunks,ner_dictionary['GPE'])\n",
      "    orgs = get_ner_organizations(nchunks, ner_fd_entities.items())\n",
      "    nchunks = filter_fd(nchunks, allcountries)\n",
      "    nchunks = filter_fd(nchunks, orgs)\n",
      "    orgs = count_fd(orgs)\n",
      "    allcountries = count_fd(allcountries)\n",
      "    return orgs,allcountries, nchunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load_countries() DONE!\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Processing NGrams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generates 5 different frequency distributions:\n",
      "* Unigrams (not very useful)\n",
      "* Unigrams with stemming (not very useful)\n",
      "* Bigrams (n=2)\n",
      "* Trigrams (n=3)\n",
      "* Quadgrams (n=4) \n",
      "\n",
      "All ngrams are alpha numeric, lowercase, and without stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_ngrams(sentences=None, sent_tokens=None, limit = 50):\n",
      "    sent_tokens = sent_tokens if sent_tokens else tokenize_sentence_text(sentences, alnum_only=True, \\\n",
      "                                                                          remove_stopwords=True, use_pattern = 2)\n",
      "    unigrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens, ngram_length = 1, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    unigrams_stem_fd = get_frequent_ngrams(sent_tokens=sent_tokens,ngram_length =  1, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True, stem_words=True)\n",
      "    bigrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens, ngram_length = 2, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    trigrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens, ngram_length = 3, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    quadgrams_fd = get_frequent_ngrams(sent_tokens=sent_tokens,ngram_length =  4, alnum_only=True, \\\n",
      "                                    remove_stopwords=True, lower_case=True)\n",
      "    return print_FreqDists([unigrams_fd,unigrams_stem_fd, bigrams_fd,trigrams_fd, quadgrams_fd],\\\n",
      "                           titles=['Unigram', 'Stemed Unigram', 'Bigrams', 'Trigrams', 'Quadgrams'], limit=limit, csv=True)\n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extract Document Links"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extracts document references from text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DOCUMENT_LINK_PATTERN = '([A-Z0-9._-]+/)+([A-z0-9._-]+)*'\n",
      "def extract_links_from_documents(docs, show_pbar=None):\n",
      "    show_pbar = show_pbar if show_pbar is not None else is_show_pbars()\n",
      "    links = {}\n",
      "    if show_pbar:\n",
      "        pbar = ProgressBar(widgets=[\"Extracting Outgoing Links \", SimpleProgress(), Percentage(), Bar(), ETA()], maxval=len(docs)).start()\n",
      "    i = 0\n",
      "    doc_ids = dict([(docs[doc]['id'], doc) for doc in docs])\n",
      "    for doc in docs:\n",
      "        links[doc] ={'outgoing':[], 'incoming':[]}\n",
      "    for doc in docs:\n",
      "        olinks= extract_links_from_document(docs[doc])\n",
      "        for olink in olinks:\n",
      "            if olink in doc_ids:\n",
      "                links[doc_ids[olink]]['incoming'].append(docs[doc]['id'])\n",
      "        links[doc]['outgoing']=olinks\n",
      "        i+=1\n",
      "        if show_pbar:\n",
      "            pbar.update(i)\n",
      "    if show_pbar:\n",
      "        pbar.finish()\n",
      "    return links\n",
      "\n",
      "\n",
      "def extract_links_from_document(doc):\n",
      "    if 'content' not in doc:\n",
      "        doc = doc.itervalues().next()\n",
      "    \n",
      "    text = \"\\n\".join([ \" \".join(c) for c in doc['content']])\n",
      "    return extract_links_from_text(text)\n",
      "\n",
      "\n",
      "def extract_links_from_text(text):\n",
      "    links = re.finditer(DOCUMENT_LINK_PATTERN, text)\n",
      "    result = [link.group(0) for link in links]\n",
      "    #check if there joint references extracted as one\n",
      "    for link in result:\n",
      "        if '-' in link:\n",
      "            sublinks = link.split('-')\n",
      "            result+= [sublink for sublink in link.split('-') if '/' in sublink]\n",
      "    \n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Get and Download Source PDF File"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse the URL for the original PDF file and download"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pattern.web import URL\n",
      "BASE_URL = 'http://documents-dds-ny.un.org/doc/'\n",
      "def get_document_url(doc_id=None, doc_name = None):\n",
      "    \n",
      "    doc = get_document(doc_id=doc_id, doc_name=doc_name).itervalues().next()\n",
      "#     print doc['scrape']\n",
      "    try:\n",
      "        n = doc['attributes']['n']\n",
      "        area = doc['scrape']['Area']\n",
      "        dist = doc['scrape']['Distribution']\n",
      "        url = '%s%s/%s/%s/%s/%s/pdf/%s.pdf?OpenElement'%(BASE_URL,area,dist,  n[:3],n[3:6], n[-2:],n)\n",
      "#         print url\n",
      "        return url\n",
      "    except Exception, e:\n",
      "        return None\n",
      "\n",
      "\n",
      "def download_file_to(source, destination):\n",
      "    name = source.split('/')[-1]\n",
      "    url = URL(source)\n",
      "    with open(os.path.join(destination, name), 'wb') as f:\n",
      "        d = url.open()\n",
      "        print d.read()\n",
      "        f.write(d.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Printing Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These functions help print outputs nicely (e.g. multiple frequency distributions side by side in a table)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def print_FreqDist(fd, limit =0):\n",
      "    if limit == 0:\n",
      "        limit = len(fd.items())\n",
      "    print \"\\n\".join([\"%d\\t%s\"%( value, word) for (word, value) in fd.items()[:limit]])\n",
      "    \n",
      "#prints multiple frequency distribution next to each other to compare results.\n",
      "def print_FreqDists(fds, titles=None, limit = 50, csv=False):\n",
      "    return print_csv_table(preprint_FreqDists(fds, titles, limit, csv=True))\n",
      "\n",
      "\n",
      "def preprint_FreqDists(fds, titles=None, limit = 50, csv=False):\n",
      "    lines = ''\n",
      "    html_str =''\n",
      "    if limit ==0:\n",
      "        limit = max([len(fd) for fd in fds])\n",
      "    titles = titles if titles else [i for i in range(len(fds))]\n",
      "    lines+=\",\".join(['%s phrase,%s frequency'%(t,t) for t in titles])+'\\n'\n",
      "    for i in range(limit):\n",
      "        line = '%d'%(i+1)\n",
      "        for fd in fds:\n",
      "            key = ''\n",
      "            val = 0\n",
      "            try:\n",
      "                if i < len(fd.items()):\n",
      "                    key =fd.items()[i][0]\n",
      "                    val = fd.items()[i][1]\n",
      "            except:\n",
      "                key = fd[i][0]\n",
      "                val = fd[i][1]\n",
      "                pass\n",
      "            if csv:\n",
      "                line='%s,\"%s\",%d'%(line,key,val)\n",
      "            else:\n",
      "                line='%s\\t%s\\t%d'%(line,key,val)\n",
      "       \n",
      "#         print line\n",
      "        lines+='%s\\n'%line\n",
      "    return lines\n",
      "\n",
      "\n",
      "def print_csv_table(csv_lines):\n",
      "    import pandas, io\n",
      "    if isinstance(csv_lines, list):\n",
      "        csv_lines = \"\\n\".join(csv_lines)\n",
      "    \n",
      "    plines= pandas.read_csv(io.BytesIO(str(csv_lines)))\n",
      "    return plines\n",
      "    \n",
      "    \n",
      "def print_pos_tagged_sentences(tagged_sentences):\n",
      "    formatted_sents = []\n",
      "    \n",
      "    import io\n",
      "    max_tokens = max([len(sent) for sent in tagged_sentences])\n",
      "    formatted_sents.append(\",\".join([' ' for i in range(max_tokens)]))\n",
      "    for sent in tagged_sentences:\n",
      "        words = ['\"%s\"'%word for (word, tag) in sent]\n",
      "        tags = ['\"%s\"'%tag for (word, tag) in sent]\n",
      "        if len(words)<max_tokens:\n",
      "            words+=[' ' for i in range(max_tokens - len(words))]\n",
      "            tags+=[' ' for i in range(max_tokens - len(tags))]\n",
      "        csv_line = \"%s\\n%s\\n%s\\n\"%(\",\".join(words), \",\".join(tags), \",\".join([' ' for i in range(max_tokens)]))\n",
      "        formatted_sents.append(','.join(words))\n",
      "        formatted_sents.append(','.join(tags))\n",
      "        formatted_sents.append( \",\".join([' ' for i in range(max_tokens)]))\n",
      "    return formatted_sents\n",
      "\n",
      "\n",
      "#allow me to limit the number of chars in the output whithout whitespace. I am using a table output that generates tons of white space\n",
      "def shrink_output_text(text, char_limit=2500, count_whitespace = False):\n",
      "    text2 = [(i, str(text[i])) for i in range(len(text)) if text[i]!=' ']\n",
      "    if count_whitespace:\n",
      "        return text[:char_limit]\n",
      "    else:\n",
      "        try:\n",
      "            last_char_tuple = text2[len(text2)-1][0] if len(text2)<=char_limit else text2[char_limit][0]\n",
      "            return text[:last_char_tuple]\n",
      "        except Exception, e:\n",
      "            print e\n",
      "            print len(text2), char_limit\n",
      "            return text[:char_limit]\n",
      "        \n",
      "def print_collocations_finders(finders, chunked=False):\n",
      "    finders = list(finders)\n",
      "    if chunked:\n",
      "        \n",
      "        for i in range(len(finders)):\n",
      "#             print finders[i][:3]\n",
      "            finders[i] = [tuple([word_tag[0] for word_tag in item]) for item in finders[i] ]\n",
      "#             print finders[i][:3]\n",
      "    output = []\n",
      "    total_width = sum([len(finder[0]) for finder in finders])\n",
      "#     print total_width\n",
      "#     output.append(','.join([' ' for i in range(total_width + len(finders))]))\n",
      "    output.append('bigram pmi, , ,bigram chi_sq, , ,trigram pmi, , , ,tirgram chi_sq, , ' )\n",
      "    for i in range(len(finders[0])):\n",
      "        line = \" ,\"\n",
      "        for finder in finders:\n",
      "            if i< len(finder):\n",
      "                line+=\",\".join(finder[i])\n",
      "            line+=\",|,\"\n",
      "        line= line[:-4]\n",
      "        output.append(line)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Generate HTML"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "\n",
      "IGNORE_LIST = ['jobs', 'Display PDF File', 'links', 'Download File', 'content' ]\n",
      "def is_heading(para):\n",
      "    return len(para)==1 and para[0][-1] not in ['.', ':', '\"', '\\'', ';', ','] and para[0][0] not in ['*', '(', '\"', '\\'']\n",
      "\n",
      "\n",
      "def json2html(obj, ignore_list = IGNORE_LIST):\n",
      "    html = '<table class=\"table\">'\n",
      "    isDict = isinstance(obj, dict)\n",
      "    i = 0\n",
      "    for o in obj:\n",
      "        key = o if isDict else i+1\n",
      "#         print type(obj[key])\n",
      "        value = obj[o] if isDict else o\n",
      "        if isinstance(value, dict):\n",
      "            value = json2html(value, ignore_list)\n",
      "        elif isinstance(value, list):\n",
      "            value =  json2html(value, ignore_list)\n",
      "        \n",
      "        if key =='link':\n",
      "            value = '<a href=\"%s\" target=\"_blank\">%s</a>'%(value, value)\n",
      "#         value = obj[o] if type(o) is str or type(o) is unicode \\\n",
      "#                 else \",\".join(obj[o]) if type(o) is list \\\n",
      "#                 else json2html(obj[o])\n",
      "        if key not in ignore_list:\n",
      "            html+='<tr><td>%s</td><td>%s</td></tr>'%(key, value)\n",
      "        i+=1\n",
      "    html+='</table>'\n",
      "    return html\n",
      "\n",
      "\n",
      "def get_doc_html_with_links(doc, url, use_doc_name=False, use_n=False):\n",
      "    links = set(extract_links_from_document(doc))\n",
      "#     print links\n",
      "    html = get_doc_html(doc)\n",
      "    linked_docs={}\n",
      "    for link in links:\n",
      "#         print link\n",
      "        link_doc =get_document(doc_id=link)\n",
      "        if link_doc is not None:\n",
      "#             print 'adding link ', link\n",
      "            linked_docs[link]=link_doc\n",
      "    for link in linked_docs:\n",
      "        ref = linked_docs[link].iterkeys().next() if use_doc_name else \\\n",
      "                linked_docs[link].itervalues().next()['attributes']['n'] if use_n \\\n",
      "                else link\n",
      "        link_href = '<a href=\"%s%s\" target=\"_blank\">%s</a>'%(url,ref,link)\n",
      "        html = html.replace(link, link_href)\n",
      "    return html\n",
      "\n",
      "\n",
      "def get_doc_html(doc):\n",
      "    html = ''\n",
      "    if 'content' not in doc:\n",
      "        doc = doc.itervalues().next()\n",
      "        \n",
      "    if 'content' in doc:\n",
      "        for para in doc['content']:\n",
      "            if is_heading(para):\n",
      "                html+='<h1>%s</h1>'%para[0]\n",
      "            else:\n",
      "                html+= '<p>%s</p>'%(\" \".join(para))\n",
      "    return html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    }
   ],
   "metadata": {}
  }
 ]
}