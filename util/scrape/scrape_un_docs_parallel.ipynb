{
 "metadata": {
  "name": "",
  "signature": "sha256:50e11f2c7930bc547fa41854f8a4e1572b159ba3e493581535e039464441f54c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file scrape_un_docs_parallel.py\n",
      "MODULES_PATH = '''../../modules/multi_un_module.py'''\n",
      "import imp\n",
      "NF = imp.load_source('multi_un_module', MODULES_PATH)\n",
      "import multi_un_module as mun\n",
      "from selenium import webdriver\n",
      "import selenium\n",
      "from selenium.webdriver.common.keys import Keys\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import multiprocessing as mp\n",
      "from datetime import date\n",
      "import json\n",
      "import os\n",
      "# import fcntl\n",
      "\n",
      "# from selenium.webdriver.chrome\n",
      "from progressbar import AnimatedMarker, Bar, BouncingBar,\\\n",
      "                            Counter, ETA, Percentage, ProgressBar, SimpleProgress, FileTransferSpeed\n",
      "\n",
      "    \n",
      "def append_to_file(filename, txt):\n",
      "    with open(filename, 'a+b') as g:\n",
      "        g.write(txt+'\\n')\n",
      "\n",
      "\n",
      "\n",
      "def scrape_item(driver, n, i):\n",
      "    data = None\n",
      "    miss_count=0\n",
      "    while miss_count<3 and data is None:\n",
      "        data = get_data(driver,  n, i)\n",
      "        miss_count+=1\n",
      "        if data is None:\n",
      "            print 'retrying', miss_count, n, i\n",
      "        else:\n",
      "            break \n",
      "    return data\n",
      "#     if data is None:\n",
      "#         return data\n",
      "#     else:\n",
      "#         result = (name, n, i,  data)\n",
      "#         my_results.append(result)\n",
      "#         append_to_file(scrape_file, n)\n",
      "        \n",
      "        \n",
      "def scrape_list(docs):\n",
      "#     docs = args[0]\n",
      "#     scrape_file = args[1]\n",
      "#     output_file = args[2]\n",
      "    \n",
      "#     print scrape_file, output_file\n",
      "    if len(docs)>0:\n",
      "        pid = os.getpid()\n",
      "        driver =webdriver.Firefox()\n",
      "        \n",
      "        count = 0\n",
      "        my_results = []\n",
      "        missing = []\n",
      "        try:\n",
      "            if len(docs)>0:\n",
      "                pbar = ProgressBar(widgets=[str(pid), ' ',SimpleProgress(),' ', Percentage(), Bar(), ETA()], maxval=len(docs)).start()\n",
      "                for (name, n, i, scrape_file, output_file, scrape_missing_file) in docs:\n",
      "            #         print name, n, i\n",
      "                    data = scrape_item(driver, n, i)\n",
      "\n",
      "                    if data is None:\n",
      "                        try:\n",
      "#                             for handle in driver.window_handles:\n",
      "#                                 print handle\n",
      "#                                 driver.switch_to_window(handle)\n",
      "                            driver.get('http://documents.un.org/default.asp')\n",
      "                            data = scrape_item(driver, n, i)\n",
      "                        except Exception, e:\n",
      "                            print 'ERROR (IN LIST):', e\n",
      "                            print 'killing driver', driver.title\n",
      "#                             driver.close()\n",
      "                            try:\n",
      "                                driver.quit()\n",
      "                            except Exception, e:\n",
      "                                print 'unable to quit driver', e\n",
      "                            driver = webdriver.Firefox()\n",
      "                            data = scrape_item(driver, n, i)\n",
      "                            \n",
      "                        if data is None or data == False:\n",
      "                            missing.append((name, n, i))\n",
      "                            append_to_file(scrape_missing_file, n)\n",
      "                    if data == False:\n",
      "                        missing.append((name, n, i))\n",
      "                        append_to_file(scrape_missing_file, n)\n",
      "                    elif data is not None:\n",
      "                        result = (name, n, i,  data)\n",
      "                        my_results.append(result)\n",
      "                        append_to_file(output_file, json.dumps(result))\n",
      "                        append_to_file(scrape_file, n)\n",
      "                    count+=1\n",
      "                    pbar.update(count)\n",
      "        except Exception, e:\n",
      "            print 'ERROR (SCRAPE_LIST): ', e\n",
      "            pass\n",
      "        finally:\n",
      "            pbar.finish()\n",
      "            driver.quit()\n",
      "            if len(missing)>0:\n",
      "                print missing\n",
      "            return my_results, missing\n",
      "\n",
      "\n",
      "def process_main_table(table):\n",
      "    result = {}\n",
      "    table_elemnents = table.find_elements_by_tag_name('tr')\n",
      "    for rows in table_elemnents:\n",
      "        cells =  rows.find_elements_by_tag_name('td')\n",
      "    #     print len(cells)\n",
      "        if len(cells)==2:\n",
      "            if cells[0].text.strip() in ['Download File', 'Display PDF File']:\n",
      "                link_elements = cells[1].find_elements_by_tag_name('a')\n",
      "                links = [{'lang':link.text.strip(), 'link':link.get_attribute('href')} for link in link_elements]\n",
      "                result[cells[0].text.strip()] = links\n",
      "            elif cells[0].text.strip() == 'Subjects':\n",
      "                result[cells[0].text.strip()] = [cell.strip() for cell in cells[1].text.split('\\n')]\n",
      "            else:\n",
      "                result[cells[0].text.strip()] = cells[1].text.strip()\n",
      "    return result\n",
      "\n",
      "\n",
      "def process_2nd_table(table):\n",
      "    result = {'jobs':[]}\n",
      "    table_elemnents = table.find_elements_by_tag_name('tr')\n",
      "    for rows in table_elemnents:\n",
      "        cells =  rows.find_elements_by_tag_name('td')\n",
      "        if len(cells)==4 and cells[0].text.strip()!='Language Info.':\n",
      "            lang = cells[1].text.strip()\n",
      "            jobno = cells[2].text.strip()\n",
      "            release = cells[3].text.strip()\n",
      "            result['jobs'].append({'lang':lang, 'jobno':jobno, 'release_date':release})\n",
      "        elif len(cells)<4:\n",
      "            result[cells[0].text.strip()] = cells[1].text.strip()\n",
      "    return result\n",
      "\n",
      "\n",
      "def get_data(driver, n, i):\n",
      "    result = None\n",
      "    try:\n",
      "        driver.get('http://documents.un.org/default.asp')\n",
      "        driver.find_element_by_xpath('/html/body/table/tbody/tr/td/table/tbody/tr[2]/td[1]/center/a[1]').click()\n",
      "        try:\n",
      "            advance_search = driver.find_element_by_name('advanced')\n",
      "        except:\n",
      "            driver.get('http://documents.un.org/welcome.asp?language=E')\n",
      "            advance_search = driver.find_element_by_name('advanced')\n",
      "\n",
      "        advance_search.click()\n",
      "\n",
      "        try:\n",
      "            jobno_box = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, \"jobno\")))\n",
      "        except:\n",
      "    #         driver.quit()\n",
      "            print '\\failed to find jobno', n, i\n",
      "            return None\n",
      "        jobno_box = driver.find_element_by_name('jobno')\n",
      "        jobno_box.clear()\n",
      "        jobno_box.send_keys(n)\n",
      "        driver.find_element_by_xpath('/html/body/form/table/tbody/tr[5]/td/table/tbody/tr/td[1]/input').click()\n",
      "        \n",
      "        try:\n",
      "            not_found = driver.find_element_by_xpath('/html/body/form/table/tbody/tr[4]/td/p/font').text.strip()\n",
      "            print not_found\n",
      "            if not_found[0] =='0':\n",
      "                print not_found\n",
      "                result = False\n",
      "                return result\n",
      "        except:\n",
      "            \n",
      "            pass\n",
      "        \n",
      "        try:\n",
      "            link = WebDriverWait(driver, 10).until(\n",
      "                    EC.presence_of_element_located((By.XPATH,'/html/body/form/table/tbody/tr[8]/td/table/tbody/tr[1]/td[2]/b/u/a' )))\n",
      "        except:\n",
      "            print '\\nfailed to find link', n, i\n",
      "            not_found = driver.find_element_by_xpath('/html/body/form/table/tbody/tr[4]/td/p/font').text.strip()\n",
      "            if not_found[0] ==0:\n",
      "                print n, ' not found, returning false'\n",
      "                result = False\n",
      "                return result\n",
      "            else:\n",
      "                return None\n",
      "        link = driver.find_element_by_xpath('/html/body/form/table/tbody/tr[8]/td/table/tbody/tr[1]/td[2]/b/u/a')\n",
      "        href = link.get_attribute('href')\n",
      "        driver.execute_script(href) \n",
      "        try:\n",
      "            main = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.NAME, \"main\")))\n",
      "        except:\n",
      "            print '\\nfailed to find frame main', n, i\n",
      "            return None\n",
      "\n",
      "\n",
      "        driver.switch_to_frame('main')\n",
      "        table = driver.find_element_by_xpath('/html/body/form/table/tbody/tr[5]/td/div/table')\n",
      "        result = process_main_table(table)\n",
      "        try:\n",
      "            table2 = driver.find_element_by_xpath('/html/body/form/table/tbody/tr[7]/td/table')\n",
      "            t2_results = process_2nd_table(table2)\n",
      "            result.update(t2_results)\n",
      "        except Exception, e:\n",
      "            print e\n",
      "            print 'no 2nd table for ', n, i\n",
      "    except Exception, e:\n",
      "        print 'ERROR (GET_DATA): ', e\n",
      "    \n",
      "    return result\n",
      "\n",
      "\n",
      "def split_list(alist, wanted_parts=1):\n",
      "    length = len(alist)\n",
      "    return [ alist[i*length // wanted_parts: (i+1)*length // wanted_parts] \n",
      "             for i in range(wanted_parts) ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    slices = 10\n",
      "    path = None\n",
      "    SCRAPE_FILE = 'scrape_progress.txt'\n",
      "    OUTPUT_FILE = 'output_progress.json'\n",
      "    META_FILE = 'docs_meta.json'\n",
      "    MISSING_FILE = 'missing.json'\n",
      "    SCRAPE_MISSING_FILE = 'scrape_missing.txt'\n",
      "    import sys, os\n",
      "    if len(sys.argv)>1:\n",
      "        slices = int(sys.argv[1])\n",
      "    if len(sys.argv)>2 and sys.argv[2] in ['all', 'missing']:\n",
      "        path = 'C:\\\\Users\\\\Hassan\\\\Documents\\\\iSchool\\\\NLP\\\\United Nations\\\\multiUN.en\\\\un\\\\xml\\\\en'  \n",
      "    \n",
      "    if not os.path.exists(SCRAPE_FILE):\n",
      "        f = file(SCRAPE_FILE,'w')\n",
      "        f.close()\n",
      "    if not os.path.exists(OUTPUT_FILE):\n",
      "        f = file(OUTPUT_FILE, 'w')\n",
      "        f.close()\n",
      "    if not os.path.exists(SCRAPE_MISSING_FILE):\n",
      "        f = file(SCRAPE_MISSING_FILE, 'w')\n",
      "        f.close()\n",
      "\n",
      "    print 'using %d processes'%slices\n",
      "    docs = None\n",
      "    results = None\n",
      "    if not os.path.exists(META_FILE):\n",
      "        if path is None:\n",
      "            docs = mun.load_xml_files_by_year(\"TOP_100\", content=False)\n",
      "        else:\n",
      "            docs = mun.load_xml_files(path=path, content=False)\n",
      "    else:\n",
      "        docs = json.load(open(META_FILE, 'rb'))\n",
      "\n",
      "    \n",
      "    doc_names = []\n",
      "    if sys.argv[2] == 'missing':\n",
      "        missing = json.load(open(MISSING_FILE, 'rb'))\n",
      "        print 'scraping missing files only', len(missing)\n",
      "        doc_names = [(doc, docs[doc]['n'], docs[doc]['id'], SCRAPE_FILE, OUTPUT_FILE, SCRAPE_MISSING_FILE) \n",
      "                     for doc in docs if docs[doc]['n'] in  missing]\n",
      "    else:\n",
      "        done_ns = []\n",
      "        with open(SCRAPE_FILE) as f:\n",
      "            done_ns = f.readlines()\n",
      "        done_ns = [str(item).strip() for item in done_ns]\n",
      "        doc_names = [(doc, docs[doc]['n'], docs[doc]['id'], SCRAPE_FILE, OUTPUT_FILE, SCRAPE_MISSING_FILE) \n",
      "                     for doc in docs if docs[doc]['n'] not in  done_ns]\n",
      "    print 'Total Items to process:', len(doc_names)\n",
      "#     docs = None\n",
      "#     doc_names = doc_names[:20]\n",
      "#     output = scrape_list(doc_names[:2])\n",
      "    doc_slices = split_list(doc_names, slices)\n",
      "    pool = mp.Pool(processes = slices)\n",
      "#     print doc_slices[0]\n",
      "\n",
      "    results = pool.map_async(scrape_list, (doc_slices))\n",
      "    all_results = results.get()\n",
      "\n",
      "    output = [item for sublist in all_results for item in sublist[0]]\n",
      "    missing =  [item for sublist in all_results for item in sublist[1]]\n",
      "    if len(missing)>0:\n",
      "        print 'missing'\n",
      "        print missing\n",
      "\n",
      "        output+=scrape_list(missing)\n",
      "\n",
      "    with open('output_all.json', 'w') as f:\n",
      "        f.write(json.dumps(output));\n",
      "#     print output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting scrape_un_docs_parallel.py\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "missing:\n",
      "failed to find link G0424501 TRANS/2005/1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %%cmd\n",
      "# python scrape_un_docs_parallel.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}